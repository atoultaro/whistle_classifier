{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training species classifier\n",
    "## Four datasets: Oswald, Gillispie, DCLDE 2011 & Watkin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from datetime import datetime\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "# from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Lambda, Flatten, MaxPooling2D, Concatenate, LSTM, Reshape, Lambda, ConvLSTM2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "# import tensorflow_addons as tfa\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow.math import l2_normalize\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "learning_rate = 1.0e-4\n",
    "conv_dim = 16\n",
    "rnn_dim = 16\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "l2_regu = 0.000\n",
    "drop_rate = 0.2\n",
    "hidden_units = 512\n",
    "fcn_dim = 512\n",
    "\n",
    "# learning_rate = 1.e-4\n",
    "# conv_dim = 64\n",
    "# rnn_dim = 16\n",
    "# pool_size = 2\n",
    "# pool_stride = 2\n",
    "# l2_regu = 0.00\n",
    "# drop_rate = 0.2\n",
    "# # drop_rate = 0.5\n",
    "# hidden_units = 512\n",
    "# fcn_dim = 512\n",
    "\n",
    "num_epoch = 200\n",
    "# batch_size = 128\n",
    "batch_size = 32  # for cnn14+attention\n",
    "\n",
    "num_species = 23\n",
    "num_patience = 20\n",
    "\n",
    "species_dict = {'NO': 0, 'BD': 1, 'MH': 2, 'CD': 3, 'STR': 4, 'SPT': 5, 'SPIN': 6, 'PLT': 7, 'RD': 8, 'RT': 9,\n",
    "                'WSD': 10, 'FKW': 11, 'BEL': 12, 'KW': 13, 'WBD': 14, 'DUSK': 15, 'FRA': 16, 'PKW': 17, 'LPLT': 18,\n",
    "                'NAR': 19, 'CLY': 20, 'SPE': 21, 'ASP': 22}\n",
    "species_list = list(species_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type_dict = {1: 'universal', 2: 'file', 3: 'encounter', 4: 'domain'}\n",
    "# data_type = 2\n",
    "\n",
    "work_path = '/home/ys587/__Data/__whistle/__whislte_30_species'\n",
    "fit_result_path =  os.path.join(work_path, '__fit_result_species')\n",
    "# feature_path = os.path.join(work_path, '__feature_species')\n",
    "feature_path = os.path.join(work_path, '__dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_files = ['oswald_orig.npz', 'gillispie_orig.npz', 'dclde2011_orig.npz', 'watkin_orig.npz']\n",
    "fea_files2 = ['oswald_aug.npz', 'gillispie_aug.npz', 'dclde2011_aug.npz', 'watkin_aug.npz']\n",
    "# t1 = np.load(os.path.join(feature_path, fea_files[0] ))\n",
    "# t2 = np.load(os.path.join(feature_path, fea_files2[0] ))\n",
    "# t1 = np.load(os.path.join(feature_path, 'gillispie_96kHz_orig.npz' ))\n",
    "# t2 = np.load(os.path.join(feature_path, 'gillispie_96kHz_aug.npz' ))\n",
    "t1 = np.load(os.path.join(feature_path, 'gillispie_48kHz_orig.npz' ))\n",
    "t2 = np.load(os.path.join(feature_path, 'gillispie_48kHz_aug.npz' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feas_orig', 'labels_orig']\n",
      "(10, 101, 128)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(t1.files)\n",
    "feas_orig = t1['feas_orig']\n",
    "labels_orig = t1['labels_orig']\n",
    "print(feas_orig.shape)\n",
    "print(labels_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feas_aug', 'labels_aug']\n",
      "(30, 101, 128)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(t2.files)\n",
    "feas_aug = t2['feas_aug']\n",
    "labels_aug = t2['labels_aug']\n",
    "print(feas_aug.shape)\n",
    "print(labels_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder based on date & time\n",
    "today = datetime.now()\n",
    "\n",
    "fit_result_path1 = os.path.join(fit_result_path, today.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "if not os.path.exists(fit_result_path1):\n",
    "    makedirs(fit_result_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = os.path.join(work_path, '__feature_species', 'feature_species.npy')\n",
    "label_file = os.path.join(work_path, '__feature_species', 'label_species.npy')\n",
    "\n",
    "if os.path.exists(feature_file) & os.path.exists(label_file):\n",
    "    print('Loading features...')\n",
    "    features_4d = np.load(feature_file)\n",
    "    labels  = np.load(label_file)\n",
    "else:\n",
    "    fea_part_list = []\n",
    "    label_part_list = []\n",
    "    for ff in range(len(fea_files)):\n",
    "        fea_curr = np.load(os.path.join(feature_path, fea_files[ff]))\n",
    "        print(fea_files[ff])\n",
    "        print(fea_curr['fea_part_4d'].shape)\n",
    "        print(fea_curr['label_part'].shape)\n",
    "        fea_part_list.append(fea_curr['fea_part_4d'])\n",
    "        label_part_list.append(fea_curr['label_part'])\n",
    "\n",
    "    features_4d = np.concatenate(fea_part_list, axis=0)\n",
    "    labels = np.concatenate(label_part_list, axis=0)\n",
    "\n",
    "    np.save(feature_file, features_4d)\n",
    "    np.save(label_file, labels)\n",
    "    del fea_part_list\n",
    "    del label_part_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature shape: '+str(features_4d.shape))\n",
    "print('label shape: '+str(labels.shape))\n",
    "\n",
    "dim_time = features_4d.shape[1]\n",
    "dim_freq = features_4d.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "features_4d, labels = shuffle(features_4d, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value, count = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.isnan(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_name = list(species_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'species': species_name, 'count': count}\n",
    "df_seltab = pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seltab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, feature, label, batch_size=32, num_classes=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = feature\n",
    "        self.X_dim = len(feature.shape)\n",
    "        self.y = to_categorical(label, num_classes)\n",
    "        self.indices = np.arange(self.y.shape[0])\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # self.index = np.arange(len(self.indices))\n",
    "        #self.df = dataframe\n",
    "        #self.indices = self.df.index.tolist()        \n",
    "        # self.x_col = x_col\n",
    "        # self.y_col = y_col\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(floor(len(self.indices)/self.batch_size))\n",
    "        # return label.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # batch = [self.indices[k] for k in index]\n",
    "        batch = list(range(index*self.batch_size, (index+1)*self.batch_size))\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        y = np.zeros((self.batch_size, self.y.shape[1]))\n",
    "        \n",
    "        if self.X_dim == 3:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :] = self.X[id, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "                \n",
    "        elif self.X_dim == 4:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2], self.X.shape[3]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :, :] = self.X[id, :, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_test, label_train, label_test = train_test_split(features_4d, labels, test_size=0.30, random_state=42)\n",
    "del features_4d\n",
    "\n",
    "fea_train2, fea_validate, label_train2, label_validate = train_test_split(fea_train, label_train, test_size=0.20, random_state=42)\n",
    "del fea_train\n",
    "\n",
    "train_generator = DataGenerator(fea_train2, label_train2, batch_size=batch_size, num_classes=num_species)\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kong's attention\n",
    "# def max_pooling(inputs, **kwargs):\n",
    "#     input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "#     return K.max(input, axis=1)\n",
    "def max_pooling(inputs, **kwargs):\n",
    "    # input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "    return K.max(inputs, axis=1)\n",
    "\n",
    "\n",
    "def average_pooling(inputs, **kwargs):\n",
    "    input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "    return K.mean(input, axis=1)\n",
    "\n",
    "\n",
    "def attention_pooling(inputs, **kwargs):\n",
    "    [out, att] = inputs\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    att = K.clip(att, epsilon, 1. - epsilon)\n",
    "    normalized_att = att / K.sum(att, axis=1)[:, None, :]\n",
    "\n",
    "    return K.sum(out * normalized_att, axis=1)\n",
    "\n",
    "\n",
    "def pooling_shape(input_shape):\n",
    "\n",
    "    if isinstance(input_shape, list):\n",
    "        (sample_num, time_steps, freq_bins) = input_shape[0]\n",
    "\n",
    "    else:\n",
    "        (sample_num, time_steps, freq_bins) = input_shape\n",
    "\n",
    "    return (sample_num, freq_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn14 attention with customized maxpooling\n",
    "def model_cnn14_attention_multi(time_steps, freq_bins, classes_num, model_type='feature_level_attention', conv_dim=64, rnn_dim=128, pool_size=2, pool_stride=2, hidden_units=512, l2_regu=0., drop_rate=0., multilabel=True):\n",
    "    # Kong's attention\n",
    "    # model_type = 'decision_level_max_pooling'  # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_average_pooling' # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_single_attention'\n",
    "    # model_type = 'decision_level_multi_attention'\n",
    "    # model_type = 'feature_level_attention'\n",
    "\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # change dimensions: samples, time, frequency, channels => samples, time, frequency*channels\n",
    "    dim_cnn = K.int_shape(y)\n",
    "    y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "\n",
    "    a1 = Dense(hidden_units)(y)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    a1 = Dropout(drop_rate)(a1)\n",
    "\n",
    "    a2 = Dense(hidden_units)(a1)\n",
    "    a2 = BatchNormalization()(a2)\n",
    "    a2 = Activation('relu')(a2)\n",
    "    a2 = Dropout(drop_rate)(a2)\n",
    "\n",
    "    a3 = Dense(hidden_units)(a2)\n",
    "    a3 = BatchNormalization()(a3)\n",
    "    a3 = Activation('relu')(a3)\n",
    "    a3 = Dropout(drop_rate)(a3)\n",
    "\n",
    "    # Pooling layers\n",
    "    if model_type == 'decision_level_max_pooling':\n",
    "        '''Global max pooling.\n",
    "\n",
    "        [1] Choi, Keunwoo, et al. \"Automatic tagging using deep convolutional \n",
    "        neural networks.\" arXiv preprint arXiv:1606.00298 (2016).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        \n",
    "        # output_layer = Lambda(\n",
    "        #    max_pooling, \n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(max_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_average_pooling':\n",
    "        '''Global average pooling.\n",
    "\n",
    "        [2] Lin, Min, et al. Qiang Chen, and Shuicheng Yan. \"Network in \n",
    "        network.\" arXiv preprint arXiv:1312.4400 (2013).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        # output_layer = Lambda(\n",
    "        #    average_pooling,\n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(average_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_single_attention':\n",
    "        '''Decision level single attention pooling.\n",
    "        [3] Kong, Qiuqiang, et al. \"Audio Set classification with attention\n",
    "        model: A probabilistic perspective.\" arXiv preprint arXiv:1711.00927\n",
    "        (2017).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att = Dense(classes_num, activation='softmax')(a3)\n",
    "        output_layer = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "    elif model_type == 'decision_level_multi_attention':\n",
    "        '''Decision level multi attention pooling.\n",
    "        [4] Yu, Changsong, et al. \"Multi-level Attention Model for Weakly\n",
    "        Supervised Audio Classification.\" arXiv preprint arXiv:1803.02353\n",
    "        (2018).\n",
    "        '''\n",
    "        cla1 = Dense(classes_num, activation='sigmoid')(a2)\n",
    "        att1 = Dense(classes_num, activation='softmax')(a2)\n",
    "        out1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla1, att1])\n",
    "\n",
    "        cla2 = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att2 = Dense(classes_num, activation='softmax')(a3)\n",
    "        out2 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla2, att2])\n",
    "\n",
    "        b1 = Concatenate(axis=-1)([out1, out2])\n",
    "        b1 = Dense(classes_num)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Activation('sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Activation('softmax')(b1)\n",
    "\n",
    "    elif model_type == 'feature_level_attention':\n",
    "        '''Feature level attention.\n",
    "        [1] Kong, Qiuqiang, et al. \"Weakly labelled audioset tagging with \n",
    "        attention neural networks.\" (2019).\n",
    "        '''\n",
    "        cla = Dense(hidden_units, activation='linear')(a3)\n",
    "        att = Dense(hidden_units, activation='sigmoid')(a3)\n",
    "        b1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "        b1 = BatchNormalization()(b1)\n",
    "        b1 = Activation(activation='relu')(b1)\n",
    "        b1 = Dropout(drop_rate)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Dense(classes_num, activation='sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Dense(classes_num, activation='softmax')(b1)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Incorrect model_type!\")\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn14 attention\n",
    "def model_cnn14_attention(time_steps, freq_bins, classes_num, model_type='feature_level_attention', conv_dim=64, rnn_dim=128, pool_size=2, pool_stride=2, hidden_units=512, l2_regu=0., drop_rate=0., multilabel=True):\n",
    "    # Kong's attention\n",
    "    # model_type = 'decision_level_max_pooling'  # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_average_pooling' # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_single_attention'\n",
    "    # model_type = 'decision_level_multi_attention'\n",
    "    # model_type = 'feature_level_attention'\n",
    "\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # change dimensions: samples, time, frequency, channels => samples, time, frequency*channels\n",
    "    dim_cnn = K.int_shape(y)\n",
    "    y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "\n",
    "    a1 = Dense(hidden_units)(y)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    a1 = Dropout(drop_rate)(a1)\n",
    "\n",
    "    a2 = Dense(hidden_units)(a1)\n",
    "    a2 = BatchNormalization()(a2)\n",
    "    a2 = Activation('relu')(a2)\n",
    "    a2 = Dropout(drop_rate)(a2)\n",
    "\n",
    "    a3 = Dense(hidden_units)(a2)\n",
    "    a3 = BatchNormalization()(a3)\n",
    "    a3 = Activation('relu')(a3)\n",
    "    a3 = Dropout(drop_rate)(a3)\n",
    "\n",
    "    # Pooling layers\n",
    "    if model_type == 'decision_level_max_pooling':\n",
    "        '''Global max pooling.\n",
    "\n",
    "        [1] Choi, Keunwoo, et al. \"Automatic tagging using deep convolutional \n",
    "        neural networks.\" arXiv preprint arXiv:1606.00298 (2016).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        \n",
    "        # output_layer = Lambda(\n",
    "        #    max_pooling, \n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(max_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_average_pooling':\n",
    "        '''Global average pooling.\n",
    "\n",
    "        [2] Lin, Min, et al. Qiang Chen, and Shuicheng Yan. \"Network in \n",
    "        network.\" arXiv preprint arXiv:1312.4400 (2013).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        # output_layer = Lambda(\n",
    "        #    average_pooling,\n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(average_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_single_attention':\n",
    "        '''Decision level single attention pooling.\n",
    "        [3] Kong, Qiuqiang, et al. \"Audio Set classification with attention\n",
    "        model: A probabilistic perspective.\" arXiv preprint arXiv:1711.00927\n",
    "        (2017).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att = Dense(classes_num, activation='softmax')(a3)\n",
    "        output_layer = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "    elif model_type == 'decision_level_multi_attention':\n",
    "        '''Decision level multi attention pooling.\n",
    "        [4] Yu, Changsong, et al. \"Multi-level Attention Model for Weakly\n",
    "        Supervised Audio Classification.\" arXiv preprint arXiv:1803.02353\n",
    "        (2018).\n",
    "        '''\n",
    "        cla1 = Dense(classes_num, activation='sigmoid')(a2)\n",
    "        att1 = Dense(classes_num, activation='softmax')(a2)\n",
    "        out1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla1, att1])\n",
    "\n",
    "        cla2 = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att2 = Dense(classes_num, activation='softmax')(a3)\n",
    "        out2 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla2, att2])\n",
    "\n",
    "        b1 = Concatenate(axis=-1)([out1, out2])\n",
    "        b1 = Dense(classes_num)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Activation('sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Activation('softmax')(b1)\n",
    "\n",
    "    elif model_type == 'feature_level_attention':\n",
    "        '''Feature level attention.\n",
    "        [1] Kong, Qiuqiang, et al. \"Weakly labelled audioset tagging with \n",
    "        attention neural networks.\" (2019).\n",
    "        '''\n",
    "        cla = Dense(hidden_units, activation='linear')(a3)\n",
    "        att = Dense(hidden_units, activation='sigmoid')(a3)\n",
    "        b1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "        b1 = BatchNormalization()(b1)\n",
    "        b1 = Activation(activation='relu')(b1)\n",
    "        b1 = Dropout(drop_rate)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Dense(classes_num, activation='sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Dense(classes_num, activation='softmax')(b1)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Incorrect model_type!\")\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn14(time_steps, freq_bins, classes_num, pool_size=2, pool_stride=2, conv_dim=16, fcn_dim=512, l2_regu=0., drop_rate=0.):\n",
    "    # input\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    \n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    \n",
    "    y = GlobalMaxPooling2D()(y)\n",
    "    \n",
    "    # FC block\n",
    "    y = Dense(fcn_dim, activation='relu', name='cnn14_fcn')(y)  # original 512\n",
    "    x = Dense(classes_num, activation='softmax')(y)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile, class weight & fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_model(model_folder, remove_others=False):\n",
    "#     model_list = glob.glob(model_folder+'/*.hdf5')\n",
    "#     model_list.sort()\n",
    "#     the_best_model = model_list[-1]\n",
    "    \n",
    "#     if remove_others==True:\n",
    "#         for mm in model_list[:-1]:\n",
    "#             os.remove(mm)\n",
    "            \n",
    "#     print(the_best_model)\n",
    "    \n",
    "#     return the_best_model\n",
    "import re\n",
    "\n",
    "def find_best_model(classifier_path, fmt='epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_\\d+.\\d{4}.hdf5', is_max=False, purge=True):\n",
    "    \"\"\"\n",
    "    Return the path to the model with the best accuracy, given the path to\n",
    "    all the trained classifiers\n",
    "    Args:\n",
    "        classifier_path: path to all the trained classifiers\n",
    "        fmt: e.g. \"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\"\n",
    "        'epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_\\d+.\\d{4}.hdf5'\n",
    "        is_max: use max; otherwise, min\n",
    "        purge: True to purge models files except the best one\n",
    "    Return:\n",
    "        the path of the model with the best accuracy\n",
    "    \"\"\"\n",
    "    # list all files ending with .hdf5\n",
    "    day_list = sorted(glob.glob(os.path.join(classifier_path + '/', '*.hdf5')))\n",
    "\n",
    "    # re the last 4 digits for accuracy\n",
    "    hdf5_filename = []\n",
    "    hdf5_accu = np.zeros(len(day_list))\n",
    "    for dd in range(len(day_list)):\n",
    "        filename = os.path.basename(day_list[dd])\n",
    "        hdf5_filename.append(filename)\n",
    "        # m = re.search(\"_F1_(0.\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"_([0-1].\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\", filename)\n",
    "        m = re.search(fmt, filename)\n",
    "        try:\n",
    "            hdf5_accu[dd] = float(m.groups()[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # select the laregest one and write to the variable classifier_file\n",
    "    if len(hdf5_accu) == 0:\n",
    "        best_model_path = ''\n",
    "        best_accu = 0\n",
    "    else:\n",
    "        if is_max is True:\n",
    "            ind_max = np.argmax(hdf5_accu)\n",
    "        else: # use min instead\n",
    "            ind_max = np.argmin(hdf5_accu)\n",
    "        best_model_path = day_list[int(ind_max)]\n",
    "        best_accu = hdf5_accu[ind_max]\n",
    "        # purge all model files except the best_model\n",
    "        if purge:\n",
    "            for ff in day_list:\n",
    "                if ff != best_model_path:\n",
    "                    os.remove(ff)\n",
    "    print('Best model:'+str(best_accu))\n",
    "    print(best_model_path)\n",
    "    return best_model_path, best_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn4 + attention\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_max_pooling', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_multi_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# vggish\n",
    "# model = model_vggish(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim)\n",
    "\n",
    "# cnn10\n",
    "# model = model_cnn10(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14\n",
    "# model = model_cnn14(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14 attention\n",
    "# model = model_cnn14_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_bigru_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, rnn_dim=rnn_dim, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "loss = CategoricalCrossentropy()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_loss\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_best_model, _ = find_best_model(fit_result_path1, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils.io import Tee\n",
    "from contextlib import closing\n",
    "\n",
    "with closing(Tee(os.path.join(fit_result_path1, 'net_architecture.txt'), \"w\", channel=\"stdout\")) as outputstream:\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    \n",
    "    # print('learning_rate = {:.2e}'.format(learning_rate))\n",
    "    print('l2_regu = {:.2e}'.format(l2_regu))\n",
    "    print('drop_rate = {:.3f}'.format(drop_rate))\n",
    "    print('conv_dim = {:d}'.format(conv_dim))\n",
    "    print('rnn_dim = {:d}'.format(rnn_dim))\n",
    "    print('pool_size = {:d}'.format(pool_size))\n",
    "    print('pool_stride = {:d}'.format(pool_stride))\n",
    "    print('hidden_units = {:d}'.format(hidden_units))\n",
    "    print('fcn_dim = {:d}'.format(fcn_dim))\n",
    "    \n",
    "    print()\n",
    "    print(the_best_model)\n",
    "    print()\n",
    "    print(classification_report(label_test, np.argmax(label_pred, axis=1), target_names=species_list))\n",
    "    print()\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1))\n",
    "    print(cm)\n",
    "\n",
    "    cm2 = cm*1.0\n",
    "\n",
    "    for ii in range(cm.shape[0]):\n",
    "        cm_row = cm[ii, :]*1.0\n",
    "        cm2[ii, :]  = cm_row / cm_row.sum()\n",
    "    print()\n",
    "    print(np.around(cm2, 3))\n",
    "\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate scheduler\n",
    "step = tf.Variable(0, trainable=False)\n",
    "boundaries = [10, 20]\n",
    "values = [1.0e-4, 3.3e-5, 1.0e-5]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "# Later, whenever we perform an optimization step, we pass in the step.\n",
    "# learning_rate = learning_rate_fn(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
