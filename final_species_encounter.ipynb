{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species classification by whistles, Oswald data\n",
    "# Expt 2: ENCOUNTER split;  cross-validation\n",
    "\n",
    "# May  12, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "# from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Lambda, Flatten, MaxPooling2D, LSTM, ConvLSTM2D, GlobalAveragePooling2D, GlobalMaxPooling2D  # Reshape, Lambda, Concatenate\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy  # CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "# import tensorflow_addons.layers.spatial_pyramid_pooling as spp\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow.math import l2_normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from lib_validation import DataGenerator, find_best_model\n",
    "from lib_model import model_cnn14_spp, model_cnn14_attention_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "learning_rate = 1.0e-3\n",
    "conv_dim = 16\n",
    "rnn_dim = 16\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "l2_regu = 0.01\n",
    "drop_rate = 0.4\n",
    "hidden_units = 256\n",
    "fcn_dim = 256\n",
    "\n",
    "# learning_rate = 1.e-4\n",
    "# conv_dim = 64\n",
    "# rnn_dim = 16\n",
    "# pool_size = 2\n",
    "# pool_stride = 2\n",
    "# l2_regu = 0.00\n",
    "# drop_rate = 0.2\n",
    "# # drop_rate = 0.5\n",
    "# hidden_units = 512\n",
    "# fcn_dim = 512\n",
    "\n",
    "num_epoch = 200\n",
    "# batch_size = 128\n",
    "# batch_size = 32  # for cnn14+attention\n",
    "batch_size = 16  # for cnn14+spp\n",
    "copies_of_aug =  10\n",
    "\n",
    "num_patience = 40\n",
    "\n",
    "num_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type_dict = {1: 'universal', 2: 'file', 3: 'encounter', 4: 'domain'}\n",
    "# data_type = 2\n",
    "\n",
    "work_path = '/home/ys587/__Data/__whistle/__whislte_30_species'\n",
    "fit_result_path =  os.path.join(work_path, '__fit_result_species')\n",
    "# feature_path = os.path.join(work_path, '__feature_species')\n",
    "feature_path = os.path.join(work_path, '__dataset/20210210')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_dict = {'BD': 0, 'CD': 1, 'STR': 2, 'SPT': 3, 'SPIN': 4, 'PLT': 5, 'RT': 6,  'FKW': 7}\n",
    "num_species = len(species_dict)\n",
    "species_list = list(species_dict.keys())\n",
    "species_id = list(species_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compile, class weight & fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = '/home/ys587/__Data/__whistle/__whislte_30_species/__dataset/__feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'all.csv', 'all_orig.npz', 'all_aug.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feas_orig: (20074, 101, 128)\n",
      "The shape of feas_aug: (200740, 101, 128)\n"
     ]
    }
   ],
   "source": [
    "# original data\n",
    "fea_temp_orig = np.load(os.path.join(feature_path, 'all_orig.npz'))\n",
    "feas_orig = fea_temp_orig['feas_orig']\n",
    "labels_orig = fea_temp_orig['labels_orig']\n",
    "print('The shape of feas_orig: ', end='')\n",
    "print(feas_orig.shape)\n",
    "\n",
    "# augmented data\n",
    "fea_temp_aug = np.load(os.path.join(feature_path, 'all_aug.npz'))\n",
    "feas_aug = fea_temp_aug['feas_aug']\n",
    "labels_aug = fea_temp_aug['labels_aug']\n",
    "print('The shape of feas_aug: ', end='')\n",
    "print(feas_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.5306255e-06, -5.5306255e-06, -5.5306255e-06, ...,\n",
       "        -5.5306255e-06, -5.5306255e-06, -5.5306255e-06],\n",
       "       [ 6.7983616e-05, -2.1578808e-05, -9.3846163e-04, ...,\n",
       "         1.3514265e-04,  1.4964730e-04, -7.9413832e-07],\n",
       "       [-1.9058969e-04,  2.9266728e-04, -8.4963525e-05, ...,\n",
       "         2.2916051e-05,  5.7070574e-06,  2.0603933e-05],\n",
       "       ...,\n",
       "       [ 1.8069613e-05, -2.1375941e-05,  3.0118985e-05, ...,\n",
       "         2.9827048e-05,  2.3533221e-06,  6.4520614e-06],\n",
       "       [-3.0250607e-05, -2.1315935e-05,  3.5012261e-05, ...,\n",
       "         3.7479298e-05, -1.9081334e-04, -2.6004465e-04],\n",
       "       [-2.0094125e-05, -1.4169634e-04,  3.3643784e-04, ...,\n",
       "         1.4158267e-05, -7.2127339e-05, -5.5356446e-05]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feas_orig[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.9602832e-05, -2.9602832e-05, -2.9602832e-05, ...,\n",
       "        -2.9602832e-05, -2.9602832e-05, -2.9602832e-05],\n",
       "       [-4.8709131e-05, -1.6511540e-04, -1.9601225e-04, ...,\n",
       "        -2.3991024e-05, -5.6935693e-05, -6.8633824e-05],\n",
       "       [-4.8636775e-05, -5.4319677e-05,  5.7387711e-05, ...,\n",
       "        -2.6437801e-05, -9.7370776e-06,  7.7130687e-07],\n",
       "       ...,\n",
       "       [-3.5884852e-05, -4.7145946e-05, -6.9007998e-05, ...,\n",
       "         8.6452183e-06,  4.7872513e-06, -5.1426055e-06],\n",
       "       [-1.5889327e-06, -6.8359346e-05, -4.4359949e-05, ...,\n",
       "        -1.3005194e-05, -2.0617288e-05, -2.1008937e-05],\n",
       "       [-4.3875840e-05, -6.4716137e-06, -1.1660403e-05, ...,\n",
       "        -2.7213528e-05,  3.3381439e-05,  7.0470451e-05]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feas_aug[1*10+0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_species = pd.read_csv(os.path.join(feature_path, 'all.csv'))\n",
    "df_species = pd.read_csv(os.path.join(feature_path, 'all_species.csv'))\n",
    "df_noise = pd.read_csv(os.path.join(feature_path, 'all_noise.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HICEAS2002_s165\n",
      "PICEAS2005_a101\n",
      "PICEAS2005_a73\n",
      "STAR2000_s352\n",
      "STAR2000_s46\n",
      "STAR2003_s494\n",
      "STAR2003_s516\n",
      "STAR2003_s586\n",
      "STAR2000_s282\n",
      "STAR2000_s302\n",
      "STAR2000_s303\n",
      "STAR2000_s313\n",
      "STAR2000_s329\n",
      "STAR2000_s338\n",
      "STAR2000_s346\n",
      "STAR2000_s368\n",
      "STAR2000_s374\n",
      "STAR2000_s375\n",
      "STAR2000_s376\n",
      "STAR2000_s377\n",
      "STAR2000_s378\n",
      "STAR2000_s48\n",
      "STAR2000_s515\n",
      "STAR2000_s561\n",
      "STAR2003_s482\n",
      "STAR2003_s489\n",
      "STAR2003_s627\n",
      "STAR2003_s628\n",
      "STAR2003_s631\n",
      "STAR2003_s640\n",
      "STAR2003_s792\n",
      "STAR2006_s216\n",
      "PICEAS2005_a215\n",
      "PICEAS2005_a245\n",
      "PICEAS2005_a249\n",
      "PICEAS2005_a250\n",
      "PICEAS2005_a253\n",
      "PICEAS2005_a57\n",
      "PICEAS2005_a67\n",
      "STAR2003_s776\n",
      "STAR2006_s128\n",
      "HICEAS2002_s219\n",
      "HICEAS2002_s228\n",
      "HICEAS2002_s261\n",
      "HICEAS2002_s317\n",
      "HICEAS2002_s318\n",
      "PICEAS2005_a86\n",
      "PICEAS2005_a93\n",
      "STAR2000_s288\n",
      "STAR2006_s142\n",
      "STAR2006_s144\n",
      "STAR2006_s145\n",
      "STAR2006_s154\n",
      "STAR2006_s156\n",
      "STAR2006_s223\n",
      "STAR2006_s230\n",
      "HICEAS2002_s125\n",
      "HICEAS2002_s167\n",
      "HICEAS2002_s194\n",
      "HICEAS2002_s234\n",
      "HICEAS2002_s245\n",
      "PICEAS2005_a178\n",
      "PICEAS2005_a179\n",
      "PICEAS2005_a23\n",
      "PICEAS2005_a75\n",
      "STAR2000_s417\n",
      "STAR2006_s112\n",
      "STAR2006_s153\n",
      "HICEAS2002_s132\n",
      "PICEAS2005_a104\n",
      "PICEAS2005_a132\n",
      "PICEAS2005_a90\n",
      "PICEAS2005_a98\n",
      "STAR2000_s111\n",
      "STAR2000_s447\n",
      "STAR2000_s459\n",
      "STAR2000_s522\n",
      "STAR2000_s63\n",
      "STAR2003_s757\n",
      "STAR2006_s110\n",
      "HICEAS2002_s129\n",
      "HICEAS2002_s138\n",
      "HICEAS2002_s195\n",
      "HICEAS2002_s280\n",
      "HICEAS2002_s296\n",
      "PICEAS2005_a116\n",
      "PICEAS2005_a117\n",
      "PICEAS2005_a122\n",
      "PICEAS2005_a25\n",
      "PICEAS2005_a2600\n",
      "PICEAS2005_a35\n",
      "STAR2000_s125\n",
      "STAR2000_s394\n",
      "STAR2000_s42\n",
      "STAR2000_s498\n",
      "STAR2003_s508\n",
      "STAR2003_s511\n",
      "STAR2006_s237\n",
      "HICEAS2002_s124\n",
      "HICEAS2002_s160\n",
      "HICEAS2002_s189\n",
      "HICEAS2002_s205\n",
      "HICEAS2002_s244\n",
      "PICEAS2005_a107\n",
      "PICEAS2005_a108\n",
      "PICEAS2005_a151\n",
      "PICEAS2005_a50\n",
      "PICEAS2005_a94\n",
      "PICEAS2005_a96\n",
      "STAR2000_s274\n",
      "STAR2000_s380\n",
      "STAR2000_s395\n",
      "STAR2000_s396\n",
      "STAR2000_s398\n",
      "STAR2000_s399\n",
      "STAR2000_s39\n",
      "STAR2000_s445\n",
      "STAR2000_s455\n",
      "STAR2000_s472\n",
      "STAR2000_s483\n",
      "STAR2000_s6\n",
      "STAR2000_s88\n",
      "STAR2000_s92\n",
      "STAR2003_s652\n",
      "STAR2003_s784\n",
      "STAR2003_s788\n",
      "STAR2006_s111\n",
      "STAR2006_s115\n",
      "STAR2006_s138\n",
      "STAR2006_s205\n",
      "STAR2006_s248\n"
     ]
    }
   ],
   "source": [
    "# generate data separated by encounters\n",
    "# use species & encounter as keys!\n",
    "species_list = []\n",
    "encounter_unique = pd.unique(df_species['encounter'])\n",
    "species_unique = []\n",
    "for ee in encounter_unique:\n",
    "    species_unique.append(df_species[df_species['encounter']==ee]['species'])\n",
    "for ii in range(len(species_unique)):\n",
    "    print(encounter_unique[ii])\n",
    "    species_name = pd.unique(species_unique[ii])\n",
    "    species_list.append(species_name[0])\n",
    "# make an dataframe consisting of encounter_unique & species_list\n",
    "df_encounter_species = pd.DataFrame({'encounter': encounter_unique, 'species': species_list})\n",
    "# df_encounter_species.to_csv(os.path.join(dataset_path, 'encounter_species'+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split over encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now()\n",
    "# create a folder based on date & time\n",
    "fit_result_path1 = os.path.join(fit_result_path, today.strftime('%Y%m%d_%H%M%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_all = []\n",
    "label_test_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fold split over encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=num_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set\n",
      "[  2   3   4   5   6   7  13  14  15  16  17  18  19  20  21  22  23  24\n",
      "  25  26  27  28  29  30  31  34  35  36  37  38  39  40  44  45  46  47\n",
      "  48  49  50  51  52  53  54  55  58  59  60  61  62  63  64  65  66  67\n",
      "  70  71  72  73  74  75  76  77  78  79  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97 105 106 107 108 109 110 111 112 113 114 115 116\n",
      " 117 118 119 120 121 122 123 124 125 126 127 128 129 130]\n",
      "test_set\n",
      "[  0   1   8   9  10  11  12  32  33  41  42  43  56  57  68  69  80  81\n",
      "  82  83  98  99 100 101 102 103 104]\n",
      "\n",
      "27\n",
      "(5040, 101, 128)\n",
      "\n",
      "\n",
      "104\n",
      "(15034, 101, 128)\n",
      "\n",
      "train_set\n",
      "[  0   1   4   5   6   7   8   9  10  11  12  18  19  20  21  22  23  24\n",
      "  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40  41  42  43\n",
      "  47  48  49  50  51  52  53  54  55  56  57  61  62  63  64  65  66  67\n",
      "  68  69  72  73  74  75  76  77  78  79  80  81  82  83  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 111 112 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130]\n",
      "test_set\n",
      "[  2   3  13  14  15  16  17  34  44  45  46  58  59  60  70  71  84  85\n",
      "  86  87 105 106 107 108 109 110]\n",
      "\n",
      "26\n",
      "(3806, 101, 128)\n",
      "\n",
      "\n",
      "105\n",
      "(16268, 101, 128)\n",
      "\n",
      "train_set\n",
      "[  0   1   2   3   6   7   8   9  10  11  12  13  14  15  16  17  22  23\n",
      "  24  25  26  27  28  29  30  31  32  33  34  37  38  39  40  41  42  43\n",
      "  44  45  46  50  51  52  53  54  55  56  57  58  59  60  64  65  66  67\n",
      "  68  69  70  71  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 117 118 119 120 121 122 123 124 125 126 127 128 129 130]\n",
      "test_set\n",
      "[  4   5  18  19  20  21  35  36  47  48  49  61  62  63  72  73  88  89\n",
      "  90  91 111 112 113 114 115 116]\n",
      "\n",
      "26\n",
      "(3518, 101, 128)\n",
      "\n",
      "\n",
      "105\n",
      "(16556, 101, 128)\n",
      "\n",
      "train_set\n",
      "[  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  27  28  29  30  31  32  33  34  35  36  39  40  41  42  43\n",
      "  44  45  46  47  48  49  53  54  55  56  57  58  59  60  61  62  63  66\n",
      "  67  68  69  70  71  72  73  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 124 125 126 127 128 129 130]\n",
      "test_set\n",
      "[  6  22  23  24  25  26  37  38  50  51  52  64  65  74  75  76  92  93\n",
      "  94 117 118 119 120 121 122 123]\n",
      "\n",
      "26\n",
      "(3792, 101, 128)\n",
      "\n",
      "\n",
      "105\n",
      "(16282, 101, 128)\n",
      "\n",
      "train_set\n",
      "[  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  32  33  34  35  36  37  38  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  56  57  58  59  60  61  62  63  64\n",
      "  65  68  69  70  71  72  73  74  75  76  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123]\n",
      "test_set\n",
      "[  7  27  28  29  30  31  39  40  53  54  55  66  67  77  78  79  95  96\n",
      "  97 124 125 126 127 128 129 130]\n",
      "\n",
      "26\n",
      "(3918, 101, 128)\n",
      "\n",
      "\n",
      "105\n",
      "(16156, 101, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold split\n",
    "for train_set, test_set in skf.split(encounter_unique, species_list):\n",
    "    print('train_set')\n",
    "    print(train_set)\n",
    "    print('test_set')\n",
    "    print(test_set)\n",
    "    \n",
    "    fea_ind_orig = []\n",
    "    fea_ind_aug = []\n",
    "    # (a) testing\n",
    "    for tt in test_set:\n",
    "        encounter_curr = df_encounter_species.iloc[tt]['encounter']\n",
    "        # print(encounter_curr)\n",
    "        df_species_test = df_species[(df_species['encounter'] == encounter_curr)]\n",
    "        # print(list(df_species_test.index))\n",
    "        fea_ind_orig += list(df_species_test.index)\n",
    "    \n",
    "    fea_ind_orig = np.array(fea_ind_orig)\n",
    "    # original features & labels\n",
    "    # fea_ind_orig = np.array(df_species_test.index)\n",
    "    fea_test = feas_orig[fea_ind_orig, :, :]\n",
    "    label_test = labels_orig[fea_ind_orig]\n",
    "    label_test = np.array([species_dict[ll] for ll in label_test])\n",
    "    print('')\n",
    "    print(len(test_set))\n",
    "    print(fea_test.shape)\n",
    "    print('')\n",
    "    \n",
    "    # (b) training\n",
    "    for tt in train_set:\n",
    "        encounter_curr = df_encounter_species.iloc[tt]['encounter']\n",
    "        # print(encounter_curr)\n",
    "        df_species_train = df_species[(df_species['encounter'] == encounter_curr)]\n",
    "        # print(list(df_species_test.index))\n",
    "        fea_ind_aug += list(df_species_train.index)\n",
    "    \n",
    "    fea_ind_aug = np.array(fea_ind_aug)\n",
    "    fea_train = feas_aug[fea_ind_aug, :, :]\n",
    "    label_train = labels_aug[fea_ind_aug]\n",
    "    label_train = np.array([species_dict[ll] for ll in label_train])\n",
    "    print('')\n",
    "    print(len(train_set))\n",
    "    print(fea_train.shape)\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "#     df_species_train = df_species[(df_species['deployment'] != ee)]\n",
    "#     print(df_species_train.shape)\n",
    "\n",
    "\n",
    "#     for ii in test_set:\n",
    "#         print(species_list[ii]+', ', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_species_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ee in ['STAR2000', 'STAR2003', 'STAR2006', 'HICEAS2002', 'PICEAS2005']:\n",
    "for ee in ['STAR2000']:\n",
    "    print(ee)\n",
    "    \n",
    "    # (a) testing\n",
    "    df_species_test = df_species[(df_species['deployment'] == ee)]\n",
    "    print(df_species_test.shape)\n",
    "    \n",
    "    # original features & labels\n",
    "    fea_ind_orig = np.array(df_species_test.index)\n",
    "    fea_test = feas_orig[fea_ind_orig, :, :]\n",
    "    label_test = labels_orig[fea_ind_orig]\n",
    "    label_test = np.array([species_dict[ll] for ll in label_test])\n",
    "    print(fea_test.shape)\n",
    "    \n",
    "    # (b) training\n",
    "    df_species_train = df_species[(df_species['deployment'] != ee)]\n",
    "    print(df_species_train.shape)\n",
    "    \n",
    "    # original features & labels\n",
    "    fea_ind_orig = np.array(df_species_train.index)\n",
    "    #  fea_orig_curr = feas_orig[fea_ind_orig, :, :]\n",
    "    # labels_orig_curr = labels_orig[fea_ind_orig]\n",
    "    # print(fea_orig_curr.shape)\n",
    "\n",
    "    # augmented features & labels\n",
    "    fea_ind_aug = []\n",
    "    for ff in list(fea_ind_orig):\n",
    "        for ii in range(10):\n",
    "            fea_ind_aug.append(ff*10+ii)\n",
    "        \n",
    "    fea_train = feas_aug[fea_ind_aug, :, :]\n",
    "    label_train = labels_aug[fea_ind_aug]\n",
    "    label_train = np.array([species_dict[ll] for ll in label_train])\n",
    "    print(fea_train.shape)    \n",
    "    \n",
    "    print('feature train shape: '+str(fea_train.shape))\n",
    "    print('feature test shape: '+str(fea_test.shape))\n",
    "    print('label train shape: '+str(label_train.shape))\n",
    "    print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "    dim_time = fea_train.shape[1]\n",
    "    dim_freq = fea_train.shape[2]\n",
    "    print('dim_time: '+str(dim_time))\n",
    "    print('dim_freq: '+str(dim_freq))\n",
    "    \n",
    "    # shuffle features & labels\n",
    "    fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "    fea_test, label_test = shuffle(fea_test, label_test, random_state=0)\n",
    "    \n",
    "    fea_train = np.expand_dims(fea_train, axis=3)\n",
    "    fea_test = np.expand_dims(fea_test, axis=3)\n",
    "    \n",
    "    fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42+4)\n",
    "\n",
    "    train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "    del fea_train\n",
    "    validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "    del fea_validate\n",
    "    \n",
    "    # deployment folder\n",
    "    fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "    if not os.path.exists(fit_result_path2):\n",
    "        makedirs(fit_result_path2)\n",
    "        \n",
    "    # class weight\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "    class_weights = dict()\n",
    "    for ii in range(num_species):\n",
    "        class_weights[ii] = weights[ii]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Training the model\n",
    "model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = categorical_crossentropy\n",
    "loss = binary_crossentropy\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)\n",
    "model = load_model(the_best_model)\n",
    "label_pred = model.predict(fea_test)\n",
    "\n",
    "label_pred_all.append(label_pred)\n",
    "label_test_all.append(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STAR2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STAR2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HICEAS2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PICEAS2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# boundaries = [20, 40, 60, 80]\n",
    "boundaries = [100, 200, 300, 400]\n",
    "values = [1.0e-3, 3.33e-4, 1.0e-4, 3.33e-5, 1.0e-5]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2000_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2003_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2006_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_HICEAS2002_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_PICEAS2005_orig.npz'))\n",
    "    \n",
    "fea_train = fea_temp['feas_orig']\n",
    "label_train_list = fea_temp['labels_orig']\n",
    "del fea_temp\n",
    "\n",
    "fea_train = fea_train[:,:100,:]\n",
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_train[:label_train_pred.shape[0]], np.argmax(label_train_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top k accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = []\n",
    "for kk in range(1, num_species+1):\n",
    "    print('k='+str(kk)+':  ')\n",
    "    this_acc = top_k_accuracy_score(label_train[:label_train_pred.shape[0]], label_train_pred, k=kk, labels=list(range(num_species)))\n",
    "    print(this_acc)\n",
    "    top_k.append(this_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.bar(list(range(1, num_species+1)), top_k)\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deployment = ['STAR2000', 'STAR2003', 'STAR2006', 'HICEAS2002', 'PICEAS2005']  # oswald_STAR2000_orig.npz, oswald_STAR2000_aug.npz\n",
    "feature_path = '/home/ys587/__Data/__whistle/__whislte_30_species/__dataset/20210223_augment_all_three_noise_mixed'\n",
    "# feature_path = '/home/ys587/__Data/__whistle/__whislte_30_species/__dataset/20210308_augment_all_three_noise_mixed_class_balanced_min_5'\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if False:\n",
    "    for ee in deployment:\n",
    "        print(ee)\n",
    "        ee_others = [ee2 for ee2 in deployment if (ee2 != ee) ]\n",
    "        fea_train_files_tot = []\n",
    "        for ee2 in ee_others:\n",
    "            fea_train_files_tot.append('oswald_'+ee2+'_orig.npz')\n",
    "            fea_train_files_tot.append('oswald_'+ee2+'_aug.npz')\n",
    "\n",
    "        # Training data\n",
    "        fea_train_list = []\n",
    "        label_train_list = []\n",
    "        for ii in range(len(fea_train_files_tot)):\n",
    "            ff = fea_train_files_tot[ii]\n",
    "            print(ff)\n",
    "            fea_temp = np.load(os.path.join(feature_path, ff))\n",
    "            print(fea_temp.files)\n",
    "\n",
    "            if ii == 0:\n",
    "                fea_train = fea_temp['feas_orig']\n",
    "                label_train = fea_temp['labels_orig']\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "            elif ii % 2 == 0:  # even\n",
    "                fea_train = np.concatenate([fea_train, fea_temp['feas_orig']])\n",
    "                label_train = np.concatenate([label_train, fea_temp['labels_orig']])\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "            else:\n",
    "                fea_train = np.concatenate([fea_train, fea_temp['feas_aug']])\n",
    "                label_train = np.concatenate([label_train, fea_temp['labels_aug']])\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "        print(fea_train.shape)\n",
    "        print(label_train.shape)\n",
    "        np.savez(os.path.join(feature_path, './train_oswald_no_'+ee+'.npz'), fea_train=fea_train, label_train=label_train)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### cnn4 + attention\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_max_pooling', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_multi_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# vggish\n",
    "# model = model_vggish(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim)\n",
    "\n",
    "# cnn10\n",
    "# model = model_cnn10(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14\n",
    "# model = model_cnn14(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14 attention\n",
    "# model = model_cnn14_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_bigru_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, rnn_dim=rnn_dim, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## STAR2000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ee = deployment[0]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp\n",
    "\n",
    "fea_train = fea_train[:,:100,:]\n",
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data cleaning. Remove those not trained well\n",
    "np.savez(os.path.join(feature_path, 'train_oswald_no_'+ee+'_label_correct.npz'), label_train=label_train, label_train_pred=label_train_pred )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = label_train[:label_train_pred.shape[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train_pred[0, :]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# testing\n",
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# >>> target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(label_test, np.argmax(label_pred, axis=1), target_names=list(range(num_species))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## top k accuracy score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "top_k = []\n",
    "for kk in range(1, num_species+1):\n",
    "    print('k='+str(kk)+':  ')\n",
    "    this_acc = top_k_accuracy_score(label_test, label_pred, k=kk, labels=list(range(num_species)))\n",
    "    print(this_acc)\n",
    "    top_k.append(this_acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.bar(list(range(1, num_species+1)), top_k)\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## average_precision_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "average_precision_score(to_categorical(label_test, num_classes=8), label_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "Counter(label_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## STAR2003"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ee = deployment[1]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## STAR2006"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ee = deployment[2]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## HICEAS2002"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ee = deployment[3]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## PICES2005"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ee = deployment[4]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
