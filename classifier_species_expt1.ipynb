{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training species classifier Expt 1\n",
    "## Trained on all except PICEAS2005 & STAR2000\n",
    "## Four datasets: Oswald, Gillispie, DCLDE 2011 & Watkin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from datetime import datetime\n",
    "\n",
    "from math import floor\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "# from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Lambda, Flatten, MaxPooling2D, Concatenate, LSTM, Reshape, Lambda, ConvLSTM2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "# import tensorflow_addons as tfa\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow.math import l2_normalize\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "learning_rate = 1.0e-4\n",
    "conv_dim = 16\n",
    "rnn_dim = 16\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "l2_regu = 0.000\n",
    "drop_rate = 0.2\n",
    "hidden_units = 512\n",
    "fcn_dim = 512\n",
    "\n",
    "# learning_rate = 1.e-4\n",
    "# conv_dim = 64\n",
    "# rnn_dim = 16\n",
    "# pool_size = 2\n",
    "# pool_stride = 2\n",
    "# l2_regu = 0.00\n",
    "# drop_rate = 0.2\n",
    "# # drop_rate = 0.5\n",
    "# hidden_units = 512\n",
    "# fcn_dim = 512\n",
    "\n",
    "num_epoch = 200\n",
    "# batch_size = 128\n",
    "batch_size = 32  # for cnn14+attention\n",
    "\n",
    "num_species = 21\n",
    "num_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type_dict = {1: 'universal', 2: 'file', 3: 'encounter', 4: 'domain'}\n",
    "# data_type = 2\n",
    "\n",
    "work_path = '/home/ys587/__Data/__whistle/__whislte_30_species'\n",
    "fit_result_path =  os.path.join(work_path, '__fit_result_species')\n",
    "# feature_path = os.path.join(work_path, '__feature_species')\n",
    "feature_path = os.path.join(work_path, '__dataset/20210209')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species_dict = {'BD': 0, 'MH': 1, 'CD': 2, 'STR': 3, 'SPT': 4, 'SPIN': 5, 'PLT': 6, 'RD': 7, 'RT': 8,\n",
    "#                 'WSD': 9, 'FKW': 10, 'BEL': 11, 'KW': 12, 'WBD': 13, 'DUSK': 14, 'FRA': 15, 'PKW': 16, 'LPLT': 17, }\n",
    "species_dict = {'BD': 0, 'MH': 1, 'CD': 2, 'STR': 3, 'SPT': 4, 'SPIN': 5, 'PLT': 6, 'RD': 7, 'RT': 8,\n",
    "                'WSD': 9, 'FKW': 10, 'BEL': 11, 'KW': 12, 'WBD': 13, 'DUSK': 14, 'FRA': 15, 'PKW': 16, 'LPLT': 17,\n",
    "                'CLY': 18, 'SPE': 19, 'ASP': 20}\n",
    "species_list = list(species_dict.keys())\n",
    "species_id = list(species_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_no_oswald_part2.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'BD': 40044,\n",
       "         'CD': 44724,\n",
       "         'FKW': 17460,\n",
       "         'PLT': 10668,\n",
       "         'RT': 2104,\n",
       "         'SPIN': 16648,\n",
       "         'SPT': 21160,\n",
       "         'STR': 17912,\n",
       "         'BEL': 1168,\n",
       "         'DUSK': 2052,\n",
       "         'FRA': 3288,\n",
       "         'KW': 8064,\n",
       "         'LPLT': 4320,\n",
       "         'PKW': 228,\n",
       "         'RD': 7412,\n",
       "         'WSD': 16676,\n",
       "         'WBD': 1608,\n",
       "         'MH': 82272,\n",
       "         'ASP': 360,\n",
       "         'CLY': 904,\n",
       "         'SPE': 27028})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_part2_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 272,\n",
       "         2.0: 776,\n",
       "         10.0: 1173,\n",
       "         6.0: 1684,\n",
       "         8.0: 795,\n",
       "         5.0: 139,\n",
       "         4.0: 944,\n",
       "         3.0: 1020})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder based on date & time\n",
    "today = datetime.now()\n",
    "\n",
    "fit_result_path1 = os.path.join(fit_result_path, today.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "if not os.path.exists(fit_result_path1):\n",
    "    makedirs(fit_result_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature train shape: (326100, 100, 128)\n",
      "feature test shape: (6803, 100, 128)\n",
      "label train shape: (326100,)\n",
      "label test shape: (6803,)\n",
      "dim_time: 100\n",
      "dim_freq: 128\n"
     ]
    }
   ],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-92b4ce70ed6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# shuffle features & labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfea_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfea_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfea_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfea_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(random_state, n_samples, *arrays)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mresample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m     return resample(*arrays, replace=False, n_samples=n_samples,\n\u001b[0m\u001b[1;32m    632\u001b[0m                     random_state=random_state)\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(replace, n_samples, random_state, stratify, *arrays)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, feature, label, batch_size=32, num_classes=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = feature\n",
    "        self.X_dim = len(feature.shape)\n",
    "        self.y = to_categorical(label, num_classes)\n",
    "        self.indices = np.arange(self.y.shape[0])\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # self.index = np.arange(len(self.indices))\n",
    "        #self.df = dataframe\n",
    "        #self.indices = self.df.index.tolist()        \n",
    "        # self.x_col = x_col\n",
    "        # self.y_col = y_col\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(floor(len(self.indices)/self.batch_size))\n",
    "        # return label.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # batch = [self.indices[k] for k in index]\n",
    "        batch = list(range(index*self.batch_size, (index+1)*self.batch_size))\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        y = np.zeros((self.batch_size, self.y.shape[1]))\n",
    "        \n",
    "        if self.X_dim == 3:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :] = self.X[id, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "                \n",
    "        elif self.X_dim == 4:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2], self.X.shape[3]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :, :] = self.X[id, :, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.30, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kong's attention\n",
    "# def max_pooling(inputs, **kwargs):\n",
    "#     input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "#     return K.max(input, axis=1)\n",
    "def max_pooling(inputs, **kwargs):\n",
    "    # input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "    return K.max(inputs, axis=1)\n",
    "\n",
    "\n",
    "def average_pooling(inputs, **kwargs):\n",
    "    input = inputs[0]   # (batch_size, time_steps, freq_bins)\n",
    "    return K.mean(input, axis=1)\n",
    "\n",
    "\n",
    "def attention_pooling(inputs, **kwargs):\n",
    "    [out, att] = inputs\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    att = K.clip(att, epsilon, 1. - epsilon)\n",
    "    normalized_att = att / K.sum(att, axis=1)[:, None, :]\n",
    "\n",
    "    return K.sum(out * normalized_att, axis=1)\n",
    "\n",
    "\n",
    "def pooling_shape(input_shape):\n",
    "\n",
    "    if isinstance(input_shape, list):\n",
    "        (sample_num, time_steps, freq_bins) = input_shape[0]\n",
    "\n",
    "    else:\n",
    "        (sample_num, time_steps, freq_bins) = input_shape\n",
    "\n",
    "    return (sample_num, freq_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn14 attention with customized maxpooling\n",
    "def model_cnn14_attention_multi(time_steps, freq_bins, classes_num, model_type='feature_level_attention', conv_dim=64, rnn_dim=128, pool_size=2, pool_stride=2, hidden_units=512, l2_regu=0., drop_rate=0., multilabel=True):\n",
    "    # Kong's attention\n",
    "    # model_type = 'decision_level_max_pooling'  # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_average_pooling' # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_single_attention'\n",
    "    # model_type = 'decision_level_multi_attention'\n",
    "    # model_type = 'feature_level_attention'\n",
    "\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # change dimensions: samples, time, frequency, channels => samples, time, frequency*channels\n",
    "    dim_cnn = K.int_shape(y)\n",
    "    y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "\n",
    "    a1 = Dense(hidden_units)(y)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    a1 = Dropout(drop_rate)(a1)\n",
    "\n",
    "    a2 = Dense(hidden_units)(a1)\n",
    "    a2 = BatchNormalization()(a2)\n",
    "    a2 = Activation('relu')(a2)\n",
    "    a2 = Dropout(drop_rate)(a2)\n",
    "\n",
    "    a3 = Dense(hidden_units)(a2)\n",
    "    a3 = BatchNormalization()(a3)\n",
    "    a3 = Activation('relu')(a3)\n",
    "    a3 = Dropout(drop_rate)(a3)\n",
    "\n",
    "    # Pooling layers\n",
    "    if model_type == 'decision_level_max_pooling':\n",
    "        '''Global max pooling.\n",
    "\n",
    "        [1] Choi, Keunwoo, et al. \"Automatic tagging using deep convolutional \n",
    "        neural networks.\" arXiv preprint arXiv:1606.00298 (2016).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        \n",
    "        # output_layer = Lambda(\n",
    "        #    max_pooling, \n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(max_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_average_pooling':\n",
    "        '''Global average pooling.\n",
    "\n",
    "        [2] Lin, Min, et al. Qiang Chen, and Shuicheng Yan. \"Network in \n",
    "        network.\" arXiv preprint arXiv:1312.4400 (2013).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        # output_layer = Lambda(\n",
    "        #    average_pooling,\n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(average_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_single_attention':\n",
    "        '''Decision level single attention pooling.\n",
    "        [3] Kong, Qiuqiang, et al. \"Audio Set classification with attention\n",
    "        model: A probabilistic perspective.\" arXiv preprint arXiv:1711.00927\n",
    "        (2017).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att = Dense(classes_num, activation='softmax')(a3)\n",
    "        output_layer = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "    elif model_type == 'decision_level_multi_attention':\n",
    "        '''Decision level multi attention pooling.\n",
    "        [4] Yu, Changsong, et al. \"Multi-level Attention Model for Weakly\n",
    "        Supervised Audio Classification.\" arXiv preprint arXiv:1803.02353\n",
    "        (2018).\n",
    "        '''\n",
    "        cla1 = Dense(classes_num, activation='sigmoid')(a2)\n",
    "        att1 = Dense(classes_num, activation='softmax')(a2)\n",
    "        out1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla1, att1])\n",
    "\n",
    "        cla2 = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att2 = Dense(classes_num, activation='softmax')(a3)\n",
    "        out2 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla2, att2])\n",
    "\n",
    "        b1 = Concatenate(axis=-1)([out1, out2])\n",
    "        b1 = Dense(classes_num)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Activation('sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Activation('softmax')(b1)\n",
    "\n",
    "    elif model_type == 'feature_level_attention':\n",
    "        '''Feature level attention.\n",
    "        [1] Kong, Qiuqiang, et al. \"Weakly labelled audioset tagging with \n",
    "        attention neural networks.\" (2019).\n",
    "        '''\n",
    "        cla = Dense(hidden_units, activation='linear')(a3)\n",
    "        att = Dense(hidden_units, activation='sigmoid')(a3)\n",
    "        b1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "        b1 = BatchNormalization()(b1)\n",
    "        b1 = Activation(activation='relu')(b1)\n",
    "        b1 = Dropout(drop_rate)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Dense(classes_num, activation='sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Dense(classes_num, activation='softmax')(b1)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Incorrect model_type!\")\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn14 attention\n",
    "def model_cnn14_attention(time_steps, freq_bins, classes_num, model_type='feature_level_attention', conv_dim=64, rnn_dim=128, pool_size=2, pool_stride=2, hidden_units=512, l2_regu=0., drop_rate=0., multilabel=True):\n",
    "    # Kong's attention\n",
    "    # model_type = 'decision_level_max_pooling'  # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_average_pooling' # problem with dimensions of the Lambda layer after training\n",
    "    # model_type = 'decision_level_single_attention'\n",
    "    # model_type = 'decision_level_multi_attention'\n",
    "    # model_type = 'feature_level_attention'\n",
    "\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # change dimensions: samples, time, frequency, channels => samples, time, frequency*channels\n",
    "    dim_cnn = K.int_shape(y)\n",
    "    y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "\n",
    "    a1 = Dense(hidden_units)(y)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    a1 = Dropout(drop_rate)(a1)\n",
    "\n",
    "    a2 = Dense(hidden_units)(a1)\n",
    "    a2 = BatchNormalization()(a2)\n",
    "    a2 = Activation('relu')(a2)\n",
    "    a2 = Dropout(drop_rate)(a2)\n",
    "\n",
    "    a3 = Dense(hidden_units)(a2)\n",
    "    a3 = BatchNormalization()(a3)\n",
    "    a3 = Activation('relu')(a3)\n",
    "    a3 = Dropout(drop_rate)(a3)\n",
    "\n",
    "    # Pooling layers\n",
    "    if model_type == 'decision_level_max_pooling':\n",
    "        '''Global max pooling.\n",
    "\n",
    "        [1] Choi, Keunwoo, et al. \"Automatic tagging using deep convolutional \n",
    "        neural networks.\" arXiv preprint arXiv:1606.00298 (2016).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        \n",
    "        # output_layer = Lambda(\n",
    "        #    max_pooling, \n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(max_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_average_pooling':\n",
    "        '''Global average pooling.\n",
    "\n",
    "        [2] Lin, Min, et al. Qiang Chen, and Shuicheng Yan. \"Network in \n",
    "        network.\" arXiv preprint arXiv:1312.4400 (2013).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        # output_layer = Lambda(\n",
    "        #    average_pooling,\n",
    "        #    output_shape=pooling_shape)(\n",
    "        #    [cla])\n",
    "        output_layer = Lambda(average_pooling)(cla)\n",
    "\n",
    "    elif model_type == 'decision_level_single_attention':\n",
    "        '''Decision level single attention pooling.\n",
    "        [3] Kong, Qiuqiang, et al. \"Audio Set classification with attention\n",
    "        model: A probabilistic perspective.\" arXiv preprint arXiv:1711.00927\n",
    "        (2017).\n",
    "        '''\n",
    "        cla = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att = Dense(classes_num, activation='softmax')(a3)\n",
    "        output_layer = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "    elif model_type == 'decision_level_multi_attention':\n",
    "        '''Decision level multi attention pooling.\n",
    "        [4] Yu, Changsong, et al. \"Multi-level Attention Model for Weakly\n",
    "        Supervised Audio Classification.\" arXiv preprint arXiv:1803.02353\n",
    "        (2018).\n",
    "        '''\n",
    "        cla1 = Dense(classes_num, activation='sigmoid')(a2)\n",
    "        att1 = Dense(classes_num, activation='softmax')(a2)\n",
    "        out1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla1, att1])\n",
    "\n",
    "        cla2 = Dense(classes_num, activation='sigmoid')(a3)\n",
    "        att2 = Dense(classes_num, activation='softmax')(a3)\n",
    "        out2 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla2, att2])\n",
    "\n",
    "        b1 = Concatenate(axis=-1)([out1, out2])\n",
    "        b1 = Dense(classes_num)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Activation('sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Activation('softmax')(b1)\n",
    "\n",
    "    elif model_type == 'feature_level_attention':\n",
    "        '''Feature level attention.\n",
    "        [1] Kong, Qiuqiang, et al. \"Weakly labelled audioset tagging with \n",
    "        attention neural networks.\" (2019).\n",
    "        '''\n",
    "        cla = Dense(hidden_units, activation='linear')(a3)\n",
    "        att = Dense(hidden_units, activation='sigmoid')(a3)\n",
    "        b1 = Lambda(\n",
    "            attention_pooling, output_shape=pooling_shape)([cla, att])\n",
    "\n",
    "        b1 = BatchNormalization()(b1)\n",
    "        b1 = Activation(activation='relu')(b1)\n",
    "        b1 = Dropout(drop_rate)(b1)\n",
    "        \n",
    "        if multilabel:\n",
    "            output_layer = Dense(classes_num, activation='sigmoid')(b1)\n",
    "        else:\n",
    "            output_layer = Dense(classes_num, activation='softmax')(b1)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Incorrect model_type!\")\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn14(time_steps, freq_bins, classes_num, pool_size=2, pool_stride=2, conv_dim=16, fcn_dim=512, l2_regu=0., drop_rate=0.):\n",
    "    # input\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    \n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 4 \n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 5\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "\n",
    "    # group 6\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*32, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    \n",
    "    y = GlobalMaxPooling2D()(y)\n",
    "    \n",
    "    # FC block\n",
    "    y = Dense(fcn_dim, activation='relu', name='cnn14_fcn')(y)  # original 512\n",
    "    x = Dense(classes_num, activation='softmax')(y)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile, class weight & fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_best_model(classifier_path, fmt='epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_\\d+.\\d{4}.hdf5', is_max=False, purge=True):\n",
    "    \"\"\"\n",
    "    Return the path to the model with the best accuracy, given the path to\n",
    "    all the trained classifiers\n",
    "    Args:\n",
    "        classifier_path: path to all the trained classifiers\n",
    "        fmt: e.g. \"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\"\n",
    "        'epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_\\d+.\\d{4}.hdf5'\n",
    "        is_max: use max; otherwise, min\n",
    "        purge: True to purge models files except the best one\n",
    "    Return:\n",
    "        the path of the model with the best accuracy\n",
    "    \"\"\"\n",
    "    # list all files ending with .hdf5\n",
    "    day_list = sorted(glob.glob(os.path.join(classifier_path + '/', '*.hdf5')))\n",
    "\n",
    "    # re the last 4 digits for accuracy\n",
    "    hdf5_filename = []\n",
    "    hdf5_accu = np.zeros(len(day_list))\n",
    "    for dd in range(len(day_list)):\n",
    "        filename = os.path.basename(day_list[dd])\n",
    "        hdf5_filename.append(filename)\n",
    "        # m = re.search(\"_F1_(0.\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"_([0-1].\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\", filename)\n",
    "        m = re.search(fmt, filename)\n",
    "        try:\n",
    "            hdf5_accu[dd] = float(m.groups()[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # select the laregest one and write to the variable classifier_file\n",
    "    if len(hdf5_accu) == 0:\n",
    "        best_model_path = ''\n",
    "        best_accu = 0\n",
    "    else:\n",
    "        if is_max is True:\n",
    "            ind_max = np.argmax(hdf5_accu)\n",
    "        else: # use min instead\n",
    "            ind_max = np.argmin(hdf5_accu)\n",
    "        best_model_path = day_list[int(ind_max)]\n",
    "        best_accu = hdf5_accu[ind_max]\n",
    "        # purge all model files except the best_model\n",
    "        if purge:\n",
    "            for ff in day_list:\n",
    "                if ff != best_model_path:\n",
    "                    os.remove(ff)\n",
    "    print('Best model:'+str(best_accu))\n",
    "    print(best_model_path)\n",
    "    return best_model_path, best_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cnn4 + attention\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_max_pooling', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_multi_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# vggish\n",
    "# model = model_vggish(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim)\n",
    "\n",
    "# cnn10\n",
    "# model = model_cnn10(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14\n",
    "# model = model_cnn14(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14 attention\n",
    "# model = model_cnn14_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_bigru_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, rnn_dim=rnn_dim, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "loss = CategoricalCrossentropy()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_loss\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_151527'\n",
    "the_best_model, _ = find_best_model(fit_result_path1, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "# cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1))\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of training data\n",
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_part1_orig.npz'))\n",
    "fea_test2 = fea_temp['feas_orig']\n",
    "label_test_list2 = fea_temp['labels_orig']\n",
    "\n",
    "fea_test2 = fea_test2[:,:100,:]\n",
    "label_test2 = np.zeros(len(label_test_list2))\n",
    "for ii in range(len(label_test_list2)):\n",
    "    label_test2[ii] = species_dict[label_test_list2[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_best_model, _ = find_best_model(fit_result_path1, purge=False)\n",
    "del model\n",
    "model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = load_model(the_best_model)\n",
    "label_pred2 = model.predict(fea_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test2, np.argmax(label_pred2, axis=1))\n",
    "    \n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.utils.io import Tee\n",
    "from contextlib import closing\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "with closing(Tee(os.path.join(fit_result_path1, 'net_architecture.txt'), \"w\", channel=\"stdout\")) as outputstream:\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    \n",
    "    # print('learning_rate = {:.2e}'.format(learning_rate))\n",
    "    print('l2_regu = {:.2e}'.format(l2_regu))\n",
    "    print('drop_rate = {:.3f}'.format(drop_rate))\n",
    "    print('conv_dim = {:d}'.format(conv_dim))\n",
    "    print('rnn_dim = {:d}'.format(rnn_dim))\n",
    "    print('pool_size = {:d}'.format(pool_size))\n",
    "    print('pool_stride = {:d}'.format(pool_stride))\n",
    "    print('hidden_units = {:d}'.format(hidden_units))\n",
    "    print('fcn_dim = {:d}'.format(fcn_dim))\n",
    "    \n",
    "    print()\n",
    "    print(the_best_model)\n",
    "    print()\n",
    "    # print(classification_report(label_test, np.argmax(label_pred, axis=1), target_names=species_id))\n",
    "    print(classification_report(label_test, np.argmax(label_pred, axis=1) ))\n",
    "    print()\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1))\n",
    "    print(cm)\n",
    "\n",
    "    cm2 = cm*1.0\n",
    "    for ii in range(cm.shape[0]):\n",
    "        cm_row = cm[ii, :]*1.0\n",
    "        \n",
    "        cm_row_sum = cm_row.sum()\n",
    "        if cm_row != 0:\n",
    "            cm2[ii, :] = cm_row / cm_row_sum\n",
    "        else:\n",
    "            cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "    print()\n",
    "    print(np.around(cm2, 3))\n",
    "\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.88      0.91      1639\n",
    "         1.0       0.00      0.00      0.00         0\n",
    "         2.0       0.97      0.89      0.93      3964\n",
    "         3.0       0.76      0.95      0.84      1512\n",
    "         4.0       0.94      0.94      0.94      1327\n",
    "         5.0       0.87      0.86      0.87       896\n",
    "         6.0       0.99      0.99      0.99       323\n",
    "         8.0       0.96      0.99      0.97       516\n",
    "         9.0       0.00      0.00      0.00         0\n",
    "        10.0       0.99      0.99      0.99      2007\n",
    "        18.0       0.00      0.00      0.00         0\n",
    "        20.0       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.93     12184\n",
    "   macro avg       0.62      0.63      0.62     12184\n",
    "weighted avg       0.93      0.93      0.93     12184\n",
    "\n",
    "\n",
    "Confusion matrix:\n",
    "[[1438    4   12   91   23   49    2    6    1   13    0    0]\n",
    " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [  23   21 3544  269   45   45    0    8    5    3    0    1]\n",
    " [   6    2   52 1436    6    7    0    2    1    0    0    0]\n",
    " [  12    0    7   39 1252   15    0    1    0    0    1    0]\n",
    " [  35    0   28   52    7  774    0    0    0    0    0    0]\n",
    " [   1    0    0    1    0    0  320    0    0    1    0    0]\n",
    " [   0    0    1    1    0    0    1  512    0    1    0    0]\n",
    " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   0    5    1    0    0    0    0    6    0 1995    0    0]\n",
    " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "\n",
    "[[0.877 0.002 0.007 0.056 0.014 0.03  0.001 0.004 0.001 0.008 0.    0.   ]\n",
    " [  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
    " [0.006 0.005 0.894 0.068 0.011 0.011 0.    0.002 0.001 0.001 0.    0.   ]\n",
    " [0.004 0.001 0.034 0.95  0.004 0.005 0.    0.001 0.001 0.    0.    0.   ]\n",
    " [0.009 0.    0.005 0.029 0.943 0.011 0.    0.001 0.    0.    0.001 0.   ]\n",
    " [0.039 0.    0.031 0.058 0.008 0.864 0.    0.    0.    0.    0.    0.   ]\n",
    " [0.003 0.    0.    0.003 0.    0.    0.991 0.    0.    0.003 0.    0.   ]\n",
    " [0.    0.    0.002 0.002 0.    0.    0.002 0.992 0.    0.002 0.    0.   ]\n",
    " [  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
    " [0.    0.002 0.    0.    0.    0.    0.    0.003 0.    0.994 0.    0.   ]\n",
    " [  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]\n",
    " [  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan]]\n",
    "Model: \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate scheduler\n",
    "step = tf.Variable(0, trainable=False)\n",
    "boundaries = [10, 20]\n",
    "values = [1.0e-4, 3.3e-5, 1.0e-5]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "# Later, whenever we perform an optimization step, we pass in the step.\n",
    "# learning_rate = learning_rate_fn(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if False:\n",
    "    # fea_files = ['oswald_part1_orig.npz', 'oswald_part1_aug.npz', 'oswald_part2_orig.npz', 'oswald_part2_aug.npz',\n",
    "    #             'gillispie_48kHz_orig.npz', 'gillispie_48kHz_aug.npz', 'gillispie_96kHz_orig.npz', 'gillispie_48kHz_aug.npz',\n",
    "    #             'dclde2011_orig.npz', 'dclde2011_aug.npz', 'watkin_orig.npz', 'watkin_aug.npz']\n",
    "\n",
    "    # Expt 1: test on oswald_part1\n",
    "    fea_train_files = ['oswald_part1_orig.npz', 'oswald_part1_aug.npz',\n",
    "                'gillispie_48kHz_orig.npz', 'gillispie_48kHz_aug.npz', 'gillispie_96kHz_orig.npz', 'gillispie_96kHz_aug.npz',\n",
    "                'dclde2011_orig.npz', 'dclde2011_aug.npz', 'watkin_orig.npz', 'watkin_aug.npz']\n",
    "\n",
    "    fea_test_files = ['oswald_part2_orig.npz']\n",
    "\n",
    "    # Training data\n",
    "    fea_train_list = []\n",
    "    label_train_list = []\n",
    "    for ii in range(len(fea_train_files)):\n",
    "        ff = fea_train_files[ii]\n",
    "        print(ff)\n",
    "        fea_temp = np.load(os.path.join(feature_path, ff))\n",
    "        print(fea_temp.files)\n",
    "\n",
    "        if ii == 0:\n",
    "            fea_train = fea_temp['feas_orig']\n",
    "            label_train = fea_temp['labels_orig']\n",
    "            print(fea_train.shape)\n",
    "            print(label_train.shape)\n",
    "        elif ii % 2 == 0:  # even\n",
    "            fea_train = np.concatenate([fea_train, fea_temp['feas_orig']])\n",
    "            label_train = np.concatenate([label_train, fea_temp['labels_orig']])\n",
    "            print(fea_train.shape)\n",
    "            print(label_train.shape)\n",
    "        else:\n",
    "            fea_train = np.concatenate([fea_train, fea_temp['feas_aug']])\n",
    "            label_train = np.concatenate([label_train, fea_temp['labels_aug']])\n",
    "            print(fea_train.shape)\n",
    "            print(label_train.shape)\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
