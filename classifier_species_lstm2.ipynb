{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training species classifier Expt 3: cross-validation of Oswald data\n",
    "# Mar 23, 2021: LSTM\n",
    "# Replace categorical cross-entropy by binary cross-entropy\n",
    "## Spatial Pyramid Pooling (SPP): [1, 1], [1, 2], [1, 4], [1, 8]\n",
    "## The augemented noise is from the all five deployments.\n",
    "## Trained on PICEAS2005 & STAR2000 whereas tested on HICEAS2002, STAR2003 & STAR2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "# from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Lambda, Flatten, MaxPooling2D, Concatenate, LSTM, Reshape, Lambda, ConvLSTM2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy  # CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons.layers.spatial_pyramid_pooling as spp\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow.math import l2_normalize\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "# learning_rate = 1.0e-3\n",
    "conv_dim = 16\n",
    "rnn_dim = 16\n",
    "pool_size = 2\n",
    "pool_stride = 1  # stride over time\n",
    "l2_regu = 0.0000\n",
    "drop_rate = 0.2\n",
    "hidden_units = 128\n",
    "fcn_dim = 128\n",
    "\n",
    "# learning_rate = 1.e-4\n",
    "# conv_dim = 64\n",
    "# rnn_dim = 16\n",
    "# pool_size = 2\n",
    "# pool_stride = 2\n",
    "# l2_regu = 0.00\n",
    "# drop_rate = 0.2\n",
    "# # drop_rate = 0.5\n",
    "# hidden_units = 512\n",
    "# fcn_dim = 512\n",
    "\n",
    "num_epoch = 400\n",
    "# batch_size = 128\n",
    "batch_size = 64  # rnn\n",
    "# batch_size = 32  # for cnn14+attention\n",
    "# batch_size = 16  # for cnn14+spp\n",
    "\n",
    "num_patience = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundaries = [20, 40, 60, 80]\n",
    "boundaries = [num_epoch, 10200, 10300, 10400]\n",
    "# values = [1.0e-3, 3.33e-4, 1.0e-4, 3.33e-5, 1.0e-5]\n",
    "values = [3.33e-3, 1.0e-3, 3.33e-4, 1.0e-4, 3.33e-5]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type_dict = {1: 'universal', 2: 'file', 3: 'encounter', 4: 'domain'}\n",
    "# data_type = 2\n",
    "\n",
    "work_path = '/home/ys587/__Data/__whistle/__whislte_30_species'\n",
    "fit_result_path =  os.path.join(work_path, '__fit_result_species')\n",
    "# feature_path = os.path.join(work_path, '__feature_species')\n",
    "feature_path = os.path.join(work_path, '__dataset/20210210')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_dict = {'BD': 0, 'CD': 1, 'STR': 2, 'SPT': 3, 'SPIN': 4, 'PLT': 5, 'RT': 6,  'FKW': 7}\n",
    "num_species = len(species_dict)\n",
    "# species_dict = {'BD': 0, 'MH': 1, 'CD': 2, 'STR': 3, 'SPT': 4, 'SPIN': 5, 'PLT': 6, 'RD': 7, 'RT': 8,\n",
    "#                 'WSD': 9, 'FKW': 10, 'BEL': 11, 'KW': 12, 'WBD': 13, 'DUSK': 14, 'FRA': 15, 'PKW': 16, 'LPLT': 17,\n",
    "#                 'CLY': 18, 'SPE': 19, 'ASP': 20}\n",
    "species_list = list(species_dict.keys())\n",
    "species_id = list(species_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ['STAR2000', 'STAR2003', 'STAR2006', 'HICEAS2002', 'PICEAS2005']  # oswald_STAR2000_orig.npz, oswald_STAR2000_aug.npz\n",
    "feature_path = '/home/ys587/__Data/__whistle/__whislte_30_species/__dataset/20210223_augment_all_three_noise_mixed'\n",
    "# feature_path = '/home/ys587/__Data/__whistle/__whislte_30_species/__dataset/20210308_augment_all_three_noise_mixed_class_balanced_min_5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for ee in deployment:\n",
    "        print(ee)\n",
    "        ee_others = [ee2 for ee2 in deployment if (ee2 != ee) ]\n",
    "        fea_train_files_tot = []\n",
    "        for ee2 in ee_others:\n",
    "            fea_train_files_tot.append('oswald_'+ee2+'_orig.npz')\n",
    "            fea_train_files_tot.append('oswald_'+ee2+'_aug.npz')\n",
    "\n",
    "        # Training data\n",
    "        fea_train_list = []\n",
    "        label_train_list = []\n",
    "        for ii in range(len(fea_train_files_tot)):\n",
    "            ff = fea_train_files_tot[ii]\n",
    "            print(ff)\n",
    "            fea_temp = np.load(os.path.join(feature_path, ff))\n",
    "            print(fea_temp.files)\n",
    "\n",
    "            if ii == 0:\n",
    "                fea_train = fea_temp['feas_orig']\n",
    "                label_train = fea_temp['labels_orig']\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "            elif ii % 2 == 0:  # even\n",
    "                fea_train = np.concatenate([fea_train, fea_temp['feas_orig']])\n",
    "                label_train = np.concatenate([label_train, fea_temp['labels_orig']])\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "            else:\n",
    "                fea_train = np.concatenate([fea_train, fea_temp['feas_aug']])\n",
    "                label_train = np.concatenate([label_train, fea_temp['labels_aug']])\n",
    "                print(fea_train.shape)\n",
    "                print(label_train.shape)\n",
    "        print(fea_train.shape)\n",
    "        print(label_train.shape)\n",
    "        np.savez(os.path.join(feature_path, './train_oswald_no_'+ee+'.npz'), fea_train=fea_train, label_train=label_train)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, feature, label, batch_size=32, num_classes=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = feature\n",
    "        self.X_dim = len(feature.shape)\n",
    "        self.y = to_categorical(label, num_classes)\n",
    "        self.indices = np.arange(self.y.shape[0])\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # self.index = np.arange(len(self.indices))\n",
    "        #self.df = dataframe\n",
    "        #self.indices = self.df.index.tolist()        \n",
    "        # self.x_col = x_col\n",
    "        # self.y_col = y_col\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(floor(len(self.indices)/self.batch_size))\n",
    "        # return label.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # batch = [self.indices[k] for k in index]\n",
    "        batch = list(range(index*self.batch_size, (index+1)*self.batch_size))\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        y = np.zeros((self.batch_size, self.y.shape[1]))\n",
    "        \n",
    "        if self.X_dim == 3:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :] = self.X[id, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "                \n",
    "        elif self.X_dim == 4:\n",
    "            X = np.zeros((self.batch_size, self.X.shape[1], self.X.shape[2], self.X.shape[3]))\n",
    "            for i, id in enumerate(batch):\n",
    "                X[i,:, :, :] = self.X[id, :, :, :]  # logic\n",
    "                y[i, :] = self.y[id, :] # labels\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN / LSTM \n",
    "def model_conv_lstm(time_steps, freq_bins, classes_num, conv_dim=64, rnn_dim=128, pool_size=2, pool_stride=2, hidden_units=512, l2_regu=0., drop_rate=0., multilabel=True):\n",
    "    input_layer = Input(shape=(time_steps, freq_bins, 1), name='input')\n",
    "    # group 1\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(input_layer)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(2, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 2\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "    # group 3\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = Conv2D(conv_dim*4, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(activation='relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(pool_size, 2), strides=(1, 2), padding='same')(y)\n",
    "    y = Dropout(drop_rate)(y)\n",
    "    \n",
    "#     # group 4 \n",
    "#     y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "#     y = Activation(activation='relu')(y)\n",
    "#     y = Conv2D(conv_dim*8, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "#     y = Activation(activation='relu')(y)\n",
    "#     y = MaxPooling2D(pool_size=(pool_size, 2), strides=(pool_stride, 2), padding='same')(y)\n",
    "#     y = Dropout(drop_rate)(y)\n",
    "\n",
    "#     # group 5\n",
    "#     y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "#     y = Activation(activation='relu')(y)\n",
    "#     y = Conv2D(conv_dim*16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(l2_regu))(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "#     y = Activation(activation='relu')(y)\n",
    "\n",
    "    \n",
    "#     dim_cnn = K.int_shape(y)\n",
    "#     y = spp.SpatialPyramidPooling2D(bins=[[time_steps, 1], [time_steps, 2], [time_steps, 4]], data_format='channels_last')(y)\n",
    "#     y = Reshape((time_steps, 7*dim_cnn[3]))(y)\n",
    "    \n",
    "    dim_cnn = K.int_shape(y)\n",
    "    y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "    \n",
    "    # y = Reshape(time_steps, 7, )(y)\n",
    "    # dim_spp = K.int_shape(y)\n",
    "    # y = Reshape((dim_spp[1]*dim_spp[2], ))(y)\n",
    "    # dim_cnn = K.int_shape(y)\n",
    "    # y = Reshape((dim_cnn[1], dim_cnn[2]*dim_cnn[3]))(y)\n",
    "    \n",
    "    \n",
    "    #y = LSTM(hidden_units, activation='relu', return_sequences=True, dropout=drop_rate, recurrent_dropout=drop_rate)(y)\n",
    "    #y = LSTM(hidden_units, activation='relu', dropout=drop_rate, recurrent_dropout=drop_rate)(y)\n",
    "    y = GRU(hidden_units, activation='relu', return_sequences=True, dropout=drop_rate)(y)\n",
    "    y = GRU(hidden_units, activation='relu', dropout=drop_rate)(y)\n",
    "    x = Dense(classes_num, activation='sigmoid')(y)\n",
    "\n",
    "#     # FC block\n",
    "#     a1 = Dense(hidden_units)(y)\n",
    "#     a1 = BatchNormalization()(a1)\n",
    "#     a1 = Activation('relu')(a1)\n",
    "#     a1 = Dropout(drop_rate)(a1)\n",
    "\n",
    "#     a2 = Dense(hidden_units)(a1)\n",
    "#     a2 = BatchNormalization()(a2)\n",
    "#     a2 = Activation('relu')(a2)\n",
    "#     a2 = Dropout(drop_rate)(a2)\n",
    "\n",
    "#     a3 = Dense(hidden_units)(a2)\n",
    "#     a3 = BatchNormalization()(a3)\n",
    "#     a3 = Activation('relu')(a3)\n",
    "#     a3 = Dropout(drop_rate)(a3)\n",
    "    \n",
    "#     y = Dense(hidden_units, activation='relu', name='cnn14_fcn')(y)  # original 512\n",
    "#      y = Dense(hidden_units, activation='relu', name='cnn14_fcn2')(y)  # original 512\n",
    "    # x = Dense(classes_num, activation='softmax')(y)\n",
    "#     x = Dense(classes_num, activation='sigmoid')(y)\n",
    "    # x = Dense(classes_num, activation='sigmoid')(a3)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile, class weight & fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_model(model_folder, remove_others=False):\n",
    "#     model_list = glob.glob(model_folder+'/*.hdf5')\n",
    "#     model_list.sort()\n",
    "#     the_best_model = model_list[-1]\n",
    "    \n",
    "#     if remove_others==True:\n",
    "#         for mm in model_list[:-1]:\n",
    "#             os.remove(mm)\n",
    "            \n",
    "#     print(the_best_model)\n",
    "    \n",
    "#     return the_best_model\n",
    "import re\n",
    "\n",
    "def find_best_model(classifier_path, fmt='epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_(\\d+.\\d{4}).hdf5', is_max=True, purge=True):\n",
    "    \"\"\"\n",
    "    Return the path to the model with the best accuracy, given the path to\n",
    "    all the trained classifiers\n",
    "    Args:\n",
    "        classifier_path: path to all the trained classifiers\n",
    "        fmt: e.g. \"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\"\n",
    "        'epoch_\\d+_valloss_(\\d+.\\d{4})_valacc_\\d+.\\d{4}.hdf5'\n",
    "        is_max: use max; otherwise, min\n",
    "        purge: True to purge models files except the best one\n",
    "    Return:\n",
    "        the path of the model with the best accuracy\n",
    "    \"\"\"\n",
    "    # list all files ending with .hdf5\n",
    "    day_list = sorted(glob.glob(os.path.join(classifier_path + '/', '*.hdf5')))\n",
    "\n",
    "    # re the last 4 digits for accuracy\n",
    "    hdf5_filename = []\n",
    "    hdf5_accu = np.zeros(len(day_list))\n",
    "    for dd in range(len(day_list)):\n",
    "        filename = os.path.basename(day_list[dd])\n",
    "        hdf5_filename.append(filename)\n",
    "        # m = re.search(\"_F1_(0.\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"_([0-1].\\d{4}).hdf5\", filename)\n",
    "        # m = re.search(\"epoch_\\d+_[0-1].\\d+_(\\d+.\\d{4}).hdf5\", filename)\n",
    "        m = re.search(fmt, filename)\n",
    "        try:\n",
    "            #  hdf5_accu[dd] = float(m.groups()[0])\n",
    "            hdf5_accu[dd] = float(m.groups()[1])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # select the laregest one and write to the variable classifier_file\n",
    "    if len(hdf5_accu) == 0:\n",
    "        best_model_path = ''\n",
    "        best_accu = 0\n",
    "    else:\n",
    "        if is_max is True:\n",
    "            ind_max = np.argmax(hdf5_accu)\n",
    "        else: # use min instead\n",
    "            ind_max = np.argmin(hdf5_accu)\n",
    "        best_model_path = day_list[int(ind_max)]\n",
    "        best_accu = hdf5_accu[ind_max]\n",
    "        # purge all model files except the best_model\n",
    "        if purge:\n",
    "            for ff in day_list:\n",
    "                if ff != best_model_path:\n",
    "                    os.remove(ff)\n",
    "    print('Best model:'+str(best_accu))\n",
    "    print(best_model_path)\n",
    "    return best_model_path, best_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cnn4 + attention\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_max_pooling', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='decision_level_multi_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# vggish\n",
    "# model = model_vggish(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim)\n",
    "\n",
    "# cnn10\n",
    "# model = model_cnn10(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14\n",
    "# model = model_cnn14(dim_time, dim_freq, num_species, conv_dim=conv_dim, fcn_dim=fcn_dim, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# cnn14 attention\n",
    "# model = model_cnn14_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_bigru_attention(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, rnn_dim=rnn_dim, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "\n",
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now()\n",
    "# create a folder based on date & time\n",
    "fit_result_path1 = os.path.join(fit_result_path, today.strftime('%Y%m%d_%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAR2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAR2000\n"
     ]
    }
   ],
   "source": [
    "ee = deployment[0]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp\n",
    "\n",
    "fea_train = fea_train[:,:100,:]\n",
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'BD': 10740,\n",
       "         'CD': 4656,\n",
       "         'FKW': 19080,\n",
       "         'SPIN': 3264,\n",
       "         'SPT': 8556,\n",
       "         'STR': 8352,\n",
       "         'PLT': 11856,\n",
       "         'RT': 7410})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 121, 1.0: 3964, 5.0: 31, 6.0: 76, 4.0: 491, 3.0: 845, 2.0: 1140})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature train shape: (73914, 100, 128)\n",
      "feature test shape: (6668, 100, 128)\n",
      "label train shape: (73914,)\n",
      "label test shape: (6668,)\n",
      "dim_time: 100\n",
      "dim_freq: 128\n"
     ]
    }
   ],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42+4)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 100, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 100, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 50, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 50, 128)           443136    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 615,928\n",
      "Trainable params: 615,480\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_conv_lstm(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = categorical_crossentropy\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.3372 - accuracy: 0.2908\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.38342, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_01_valloss_0.3025_valacc_0.3834.hdf5\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.3372 - accuracy: 0.2908 - val_loss: 0.3025 - val_accuracy: 0.3834\n",
      "Epoch 2/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.4713\n",
      "Epoch 00002: val_accuracy improved from 0.38342 to 0.43804, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_02_valloss_0.2890_valacc_0.4380.hdf5\n",
      "1039/1039 [==============================] - 271s 260ms/step - loss: 0.2788 - accuracy: 0.4713 - val_loss: 0.2890 - val_accuracy: 0.4380\n",
      "Epoch 3/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.5247\n",
      "Epoch 00003: val_accuracy improved from 0.43804 to 0.46522, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_03_valloss_0.2681_valacc_0.4652.hdf5\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.2566 - accuracy: 0.5247 - val_loss: 0.2681 - val_accuracy: 0.4652\n",
      "Epoch 4/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.5533\n",
      "Epoch 00004: val_accuracy improved from 0.46522 to 0.51345, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_04_valloss_0.2478_valacc_0.5135.hdf5\n",
      "1039/1039 [==============================] - 270s 260ms/step - loss: 0.2446 - accuracy: 0.5533 - val_loss: 0.2478 - val_accuracy: 0.5135\n",
      "Epoch 5/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.5756\n",
      "Epoch 00005: val_accuracy improved from 0.51345 to 0.56821, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_05_valloss_0.2270_valacc_0.5682.hdf5\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.2354 - accuracy: 0.5756 - val_loss: 0.2270 - val_accuracy: 0.5682\n",
      "Epoch 6/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.5997\n",
      "Epoch 00006: val_accuracy improved from 0.56821 to 0.56929, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_06_valloss_0.2326_valacc_0.5693.hdf5\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.2251 - accuracy: 0.5997 - val_loss: 0.2326 - val_accuracy: 0.5693\n",
      "Epoch 7/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.6171\n",
      "Epoch 00007: val_accuracy improved from 0.56929 to 0.58587, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_07_valloss_0.2199_valacc_0.5859.hdf5\n",
      "1039/1039 [==============================] - 269s 259ms/step - loss: 0.2175 - accuracy: 0.6171 - val_loss: 0.2199 - val_accuracy: 0.5859\n",
      "Epoch 8/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.6310\n",
      "Epoch 00008: val_accuracy did not improve from 0.58587\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.2102 - accuracy: 0.6310 - val_loss: 0.2919 - val_accuracy: 0.4760\n",
      "Epoch 9/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.6443\n",
      "Epoch 00009: val_accuracy improved from 0.58587 to 0.62011, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_09_valloss_0.2083_valacc_0.6201.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.2044 - accuracy: 0.6443 - val_loss: 0.2083 - val_accuracy: 0.6201\n",
      "Epoch 10/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.6567\n",
      "Epoch 00010: val_accuracy improved from 0.62011 to 0.65815, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_10_valloss_0.1884_valacc_0.6582.hdf5\n",
      "1039/1039 [==============================] - 268s 257ms/step - loss: 0.1982 - accuracy: 0.6567 - val_loss: 0.1884 - val_accuracy: 0.6582\n",
      "Epoch 11/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.6894\n",
      "Epoch 00011: val_accuracy improved from 0.65815 to 0.66046, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_11_valloss_0.1885_valacc_0.6605.hdf5\n",
      "1039/1039 [==============================] - 268s 258ms/step - loss: 0.1803 - accuracy: 0.6894 - val_loss: 0.1885 - val_accuracy: 0.6605\n",
      "Epoch 12/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.6936\n",
      "Epoch 00012: val_accuracy improved from 0.66046 to 0.66128, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_12_valloss_0.1881_valacc_0.6613.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1778 - accuracy: 0.6936 - val_loss: 0.1881 - val_accuracy: 0.6613\n",
      "Epoch 13/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.6936\n",
      "Epoch 00013: val_accuracy improved from 0.66128 to 0.66372, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_13_valloss_0.1880_valacc_0.6637.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1770 - accuracy: 0.6936 - val_loss: 0.1880 - val_accuracy: 0.6637\n",
      "Epoch 14/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.6991\n",
      "Epoch 00014: val_accuracy did not improve from 0.66372\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1755 - accuracy: 0.6991 - val_loss: 0.1884 - val_accuracy: 0.6625\n",
      "Epoch 15/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.6979\n",
      "Epoch 00015: val_accuracy improved from 0.66372 to 0.66685, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_15_valloss_0.1866_valacc_0.6668.hdf5\n",
      "1039/1039 [==============================] - 268s 258ms/step - loss: 0.1750 - accuracy: 0.6979 - val_loss: 0.1866 - val_accuracy: 0.6668\n",
      "Epoch 16/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.6997\n",
      "Epoch 00016: val_accuracy did not improve from 0.66685\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1737 - accuracy: 0.6997 - val_loss: 0.1872 - val_accuracy: 0.6644\n",
      "Epoch 17/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.7003\n",
      "Epoch 00017: val_accuracy improved from 0.66685 to 0.66834, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_17_valloss_0.1862_valacc_0.6683.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1738 - accuracy: 0.7003 - val_loss: 0.1862 - val_accuracy: 0.6683\n",
      "Epoch 18/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.7021\n",
      "Epoch 00018: val_accuracy improved from 0.66834 to 0.67011, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_18_valloss_0.1839_valacc_0.6701.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1729 - accuracy: 0.7021 - val_loss: 0.1839 - val_accuracy: 0.6701\n",
      "Epoch 19/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.7031\n",
      "Epoch 00019: val_accuracy did not improve from 0.67011\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1726 - accuracy: 0.7031 - val_loss: 0.1857 - val_accuracy: 0.6677\n",
      "Epoch 20/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.7050\n",
      "Epoch 00020: val_accuracy did not improve from 0.67011\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1711 - accuracy: 0.7050 - val_loss: 0.1868 - val_accuracy: 0.6643\n",
      "Epoch 21/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.7058\n",
      "Epoch 00021: val_accuracy did not improve from 0.67011\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1706 - accuracy: 0.7058 - val_loss: 0.1838 - val_accuracy: 0.6700\n",
      "Epoch 22/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.7058\n",
      "Epoch 00022: val_accuracy did not improve from 0.67011\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1704 - accuracy: 0.7058 - val_loss: 0.1858 - val_accuracy: 0.6681\n",
      "Epoch 23/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.7046\n",
      "Epoch 00023: val_accuracy improved from 0.67011 to 0.67147, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_23_valloss_0.1832_valacc_0.6715.hdf5\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1701 - accuracy: 0.7046 - val_loss: 0.1832 - val_accuracy: 0.6715\n",
      "Epoch 24/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.7097\n",
      "Epoch 00024: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1688 - accuracy: 0.7097 - val_loss: 0.1849 - val_accuracy: 0.6681\n",
      "Epoch 25/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.7089\n",
      "Epoch 00025: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1688 - accuracy: 0.7089 - val_loss: 0.1851 - val_accuracy: 0.6698\n",
      "Epoch 26/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.7097\n",
      "Epoch 00026: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1686 - accuracy: 0.7097 - val_loss: 0.1845 - val_accuracy: 0.6689\n",
      "Epoch 27/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.7096\n",
      "Epoch 00027: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1685 - accuracy: 0.7096 - val_loss: 0.1832 - val_accuracy: 0.6709\n",
      "Epoch 28/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.7095\n",
      "Epoch 00028: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 267s 257ms/step - loss: 0.1674 - accuracy: 0.7095 - val_loss: 0.1835 - val_accuracy: 0.6704\n",
      "Epoch 29/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.7121\n",
      "Epoch 00029: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 269s 259ms/step - loss: 0.1675 - accuracy: 0.7121 - val_loss: 0.1845 - val_accuracy: 0.6683\n",
      "Epoch 30/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.7132\n",
      "Epoch 00030: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 277s 266ms/step - loss: 0.1668 - accuracy: 0.7132 - val_loss: 0.1841 - val_accuracy: 0.6685\n",
      "Epoch 31/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1662 - accuracy: 0.7146\n",
      "Epoch 00031: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 272s 261ms/step - loss: 0.1662 - accuracy: 0.7146 - val_loss: 0.1838 - val_accuracy: 0.6713\n",
      "Epoch 32/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.7134\n",
      "Epoch 00032: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1657 - accuracy: 0.7134 - val_loss: 0.1838 - val_accuracy: 0.6702\n",
      "Epoch 33/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.7137\n",
      "Epoch 00033: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 272s 262ms/step - loss: 0.1654 - accuracy: 0.7137 - val_loss: 0.1848 - val_accuracy: 0.6679\n",
      "Epoch 34/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.7158\n",
      "Epoch 00034: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1648 - accuracy: 0.7158 - val_loss: 0.1836 - val_accuracy: 0.6711\n",
      "Epoch 35/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.7154\n",
      "Epoch 00035: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1645 - accuracy: 0.7154 - val_loss: 0.1854 - val_accuracy: 0.6685\n",
      "Epoch 36/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.7165\n",
      "Epoch 00036: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1642 - accuracy: 0.7165 - val_loss: 0.1845 - val_accuracy: 0.6682\n",
      "Epoch 37/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.7181\n",
      "Epoch 00037: val_accuracy did not improve from 0.67147\n",
      "1039/1039 [==============================] - 272s 261ms/step - loss: 0.1638 - accuracy: 0.7181 - val_loss: 0.1844 - val_accuracy: 0.6692\n",
      "Epoch 38/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.7181\n",
      "Epoch 00038: val_accuracy improved from 0.67147 to 0.67215, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_38_valloss_0.1834_valacc_0.6721.hdf5\n",
      "1039/1039 [==============================] - 269s 259ms/step - loss: 0.1631 - accuracy: 0.7181 - val_loss: 0.1834 - val_accuracy: 0.6721\n",
      "Epoch 39/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.7193\n",
      "Epoch 00039: val_accuracy did not improve from 0.67215\n",
      "1039/1039 [==============================] - 268s 258ms/step - loss: 0.1632 - accuracy: 0.7193 - val_loss: 0.1833 - val_accuracy: 0.6720\n",
      "Epoch 40/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.7186\n",
      "Epoch 00040: val_accuracy did not improve from 0.67215\n",
      "1039/1039 [==============================] - 272s 262ms/step - loss: 0.1626 - accuracy: 0.7186 - val_loss: 0.1828 - val_accuracy: 0.6717\n",
      "Epoch 41/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.7194\n",
      "Epoch 00041: val_accuracy did not improve from 0.67215\n",
      "1039/1039 [==============================] - 270s 260ms/step - loss: 0.1622 - accuracy: 0.7194 - val_loss: 0.1853 - val_accuracy: 0.6697\n",
      "Epoch 42/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.7196\n",
      "Epoch 00042: val_accuracy improved from 0.67215 to 0.67446, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_42_valloss_0.1839_valacc_0.6745.hdf5\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1621 - accuracy: 0.7196 - val_loss: 0.1839 - val_accuracy: 0.6745\n",
      "Epoch 43/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.7218\n",
      "Epoch 00043: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1612 - accuracy: 0.7218 - val_loss: 0.1846 - val_accuracy: 0.6692\n",
      "Epoch 44/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.7215\n",
      "Epoch 00044: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1614 - accuracy: 0.7215 - val_loss: 0.1832 - val_accuracy: 0.6713\n",
      "Epoch 45/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.7231\n",
      "Epoch 00045: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 271s 261ms/step - loss: 0.1601 - accuracy: 0.7231 - val_loss: 0.1849 - val_accuracy: 0.6692\n",
      "Epoch 46/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.7206\n",
      "Epoch 00046: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 272s 262ms/step - loss: 0.1607 - accuracy: 0.7206 - val_loss: 0.1825 - val_accuracy: 0.6745\n",
      "Epoch 47/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.7239\n",
      "Epoch 00047: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 273s 262ms/step - loss: 0.1601 - accuracy: 0.7239 - val_loss: 0.1831 - val_accuracy: 0.6740\n",
      "Epoch 48/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.7234\n",
      "Epoch 00048: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1598 - accuracy: 0.7234 - val_loss: 0.1857 - val_accuracy: 0.6670\n",
      "Epoch 49/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.7228\n",
      "Epoch 00049: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 276s 266ms/step - loss: 0.1594 - accuracy: 0.7228 - val_loss: 0.1842 - val_accuracy: 0.6685\n",
      "Epoch 50/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.7253\n",
      "Epoch 00050: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1594 - accuracy: 0.7253 - val_loss: 0.1834 - val_accuracy: 0.6713\n",
      "Epoch 51/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.7244\n",
      "Epoch 00051: val_accuracy did not improve from 0.67446\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1591 - accuracy: 0.7244 - val_loss: 0.1841 - val_accuracy: 0.6712\n",
      "Epoch 52/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.7270\n",
      "Epoch 00052: val_accuracy improved from 0.67446 to 0.67486, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_52_valloss_0.1821_valacc_0.6749.hdf5\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1582 - accuracy: 0.7270 - val_loss: 0.1821 - val_accuracy: 0.6749\n",
      "Epoch 53/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.7276\n",
      "Epoch 00053: val_accuracy improved from 0.67486 to 0.67514, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_53_valloss_0.1818_valacc_0.6751.hdf5\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1579 - accuracy: 0.7276 - val_loss: 0.1818 - val_accuracy: 0.6751\n",
      "Epoch 54/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.7263\n",
      "Epoch 00054: val_accuracy improved from 0.67514 to 0.67677, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_54_valloss_0.1816_valacc_0.6768.hdf5\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1576 - accuracy: 0.7263 - val_loss: 0.1816 - val_accuracy: 0.6768\n",
      "Epoch 55/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.7269\n",
      "Epoch 00055: val_accuracy did not improve from 0.67677\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1573 - accuracy: 0.7269 - val_loss: 0.1818 - val_accuracy: 0.6764\n",
      "Epoch 56/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.7293\n",
      "Epoch 00056: val_accuracy did not improve from 0.67677\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1569 - accuracy: 0.7293 - val_loss: 0.1847 - val_accuracy: 0.6685\n",
      "Epoch 57/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.7304\n",
      "Epoch 00057: val_accuracy did not improve from 0.67677\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1564 - accuracy: 0.7304 - val_loss: 0.1831 - val_accuracy: 0.6720\n",
      "Epoch 58/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.7309\n",
      "Epoch 00058: val_accuracy did not improve from 0.67677\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1561 - accuracy: 0.7309 - val_loss: 0.1821 - val_accuracy: 0.6751\n",
      "Epoch 59/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.7304\n",
      "Epoch 00059: val_accuracy did not improve from 0.67677\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1557 - accuracy: 0.7304 - val_loss: 0.1827 - val_accuracy: 0.6723\n",
      "Epoch 60/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.7304\n",
      "Epoch 00060: val_accuracy improved from 0.67677 to 0.67785, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_60_valloss_0.1814_valacc_0.6779.hdf5\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1556 - accuracy: 0.7304 - val_loss: 0.1814 - val_accuracy: 0.6779\n",
      "Epoch 61/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.7319\n",
      "Epoch 00061: val_accuracy did not improve from 0.67785\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1553 - accuracy: 0.7319 - val_loss: 0.1833 - val_accuracy: 0.6731\n",
      "Epoch 62/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.7315\n",
      "Epoch 00062: val_accuracy improved from 0.67785 to 0.67921, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_62_valloss_0.1806_valacc_0.6792.hdf5\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1548 - accuracy: 0.7315 - val_loss: 0.1806 - val_accuracy: 0.6792\n",
      "Epoch 63/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.7327\n",
      "Epoch 00063: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1543 - accuracy: 0.7327 - val_loss: 0.1823 - val_accuracy: 0.6747\n",
      "Epoch 64/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.7332\n",
      "Epoch 00064: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1543 - accuracy: 0.7332 - val_loss: 0.1834 - val_accuracy: 0.6704\n",
      "Epoch 65/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.7342\n",
      "Epoch 00065: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1539 - accuracy: 0.7342 - val_loss: 0.1819 - val_accuracy: 0.6760\n",
      "Epoch 66/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.7346\n",
      "Epoch 00066: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1537 - accuracy: 0.7346 - val_loss: 0.1814 - val_accuracy: 0.6777\n",
      "Epoch 67/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1530 - accuracy: 0.7355\n",
      "Epoch 00067: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 273s 263ms/step - loss: 0.1530 - accuracy: 0.7355 - val_loss: 0.1836 - val_accuracy: 0.6728\n",
      "Epoch 68/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.7342\n",
      "Epoch 00068: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1531 - accuracy: 0.7342 - val_loss: 0.1801 - val_accuracy: 0.6791\n",
      "Epoch 69/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.7375\n",
      "Epoch 00069: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1525 - accuracy: 0.7375 - val_loss: 0.1834 - val_accuracy: 0.6715\n",
      "Epoch 70/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.7353\n",
      "Epoch 00070: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1522 - accuracy: 0.7353 - val_loss: 0.1831 - val_accuracy: 0.6740\n",
      "Epoch 71/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.7367\n",
      "Epoch 00071: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1520 - accuracy: 0.7367 - val_loss: 0.1829 - val_accuracy: 0.6736\n",
      "Epoch 72/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.7379\n",
      "Epoch 00072: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1513 - accuracy: 0.7379 - val_loss: 0.1820 - val_accuracy: 0.6746\n",
      "Epoch 73/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.7377\n",
      "Epoch 00073: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 263ms/step - loss: 0.1514 - accuracy: 0.7377 - val_loss: 0.1822 - val_accuracy: 0.6789\n",
      "Epoch 74/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.7386\n",
      "Epoch 00074: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1509 - accuracy: 0.7386 - val_loss: 0.1839 - val_accuracy: 0.6726\n",
      "Epoch 75/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.7382\n",
      "Epoch 00075: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1501 - accuracy: 0.7382 - val_loss: 0.1812 - val_accuracy: 0.6769\n",
      "Epoch 76/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.7408\n",
      "Epoch 00076: val_accuracy did not improve from 0.67921\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1500 - accuracy: 0.7408 - val_loss: 0.1832 - val_accuracy: 0.6749\n",
      "Epoch 77/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.7388\n",
      "Epoch 00077: val_accuracy improved from 0.67921 to 0.67948, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_77_valloss_0.1822_valacc_0.6795.hdf5\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1495 - accuracy: 0.7388 - val_loss: 0.1822 - val_accuracy: 0.6795\n",
      "Epoch 78/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.7407\n",
      "Epoch 00078: val_accuracy did not improve from 0.67948\n",
      "1039/1039 [==============================] - 274s 264ms/step - loss: 0.1494 - accuracy: 0.7407 - val_loss: 0.1825 - val_accuracy: 0.6774\n",
      "Epoch 79/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.7414\n",
      "Epoch 00079: val_accuracy improved from 0.67948 to 0.68057, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_79_valloss_0.1811_valacc_0.6806.hdf5\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1489 - accuracy: 0.7414 - val_loss: 0.1811 - val_accuracy: 0.6806\n",
      "Epoch 80/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.7425\n",
      "Epoch 00080: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1484 - accuracy: 0.7425 - val_loss: 0.1844 - val_accuracy: 0.6736\n",
      "Epoch 81/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.7411\n",
      "Epoch 00081: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1488 - accuracy: 0.7411 - val_loss: 0.1820 - val_accuracy: 0.6806\n",
      "Epoch 82/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.7431\n",
      "Epoch 00082: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 275s 264ms/step - loss: 0.1480 - accuracy: 0.7431 - val_loss: 0.1836 - val_accuracy: 0.6774\n",
      "Epoch 83/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.7455\n",
      "Epoch 00083: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 276s 265ms/step - loss: 0.1472 - accuracy: 0.7455 - val_loss: 0.1817 - val_accuracy: 0.6783\n",
      "Epoch 84/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.7457\n",
      "Epoch 00084: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1469 - accuracy: 0.7457 - val_loss: 0.1820 - val_accuracy: 0.6793\n",
      "Epoch 85/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.7447\n",
      "Epoch 00085: val_accuracy did not improve from 0.68057\n",
      "1039/1039 [==============================] - 276s 265ms/step - loss: 0.1468 - accuracy: 0.7447 - val_loss: 0.1823 - val_accuracy: 0.6792\n",
      "Epoch 86/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.7458\n",
      "Epoch 00086: val_accuracy improved from 0.68057 to 0.68152, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_86_valloss_0.1816_valacc_0.6815.hdf5\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1465 - accuracy: 0.7458 - val_loss: 0.1816 - val_accuracy: 0.6815\n",
      "Epoch 87/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.7453\n",
      "Epoch 00087: val_accuracy did not improve from 0.68152\n",
      "1039/1039 [==============================] - 276s 265ms/step - loss: 0.1464 - accuracy: 0.7453 - val_loss: 0.1843 - val_accuracy: 0.6753\n",
      "Epoch 88/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.7460\n",
      "Epoch 00088: val_accuracy improved from 0.68152 to 0.68492, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_88_valloss_0.1820_valacc_0.6849.hdf5\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1461 - accuracy: 0.7460 - val_loss: 0.1820 - val_accuracy: 0.6849\n",
      "Epoch 89/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.7469\n",
      "Epoch 00089: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 276s 265ms/step - loss: 0.1456 - accuracy: 0.7469 - val_loss: 0.1800 - val_accuracy: 0.6848\n",
      "Epoch 90/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.7481\n",
      "Epoch 00090: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 276s 265ms/step - loss: 0.1451 - accuracy: 0.7481 - val_loss: 0.1851 - val_accuracy: 0.6774\n",
      "Epoch 91/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.7494\n",
      "Epoch 00091: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 276s 266ms/step - loss: 0.1449 - accuracy: 0.7494 - val_loss: 0.1819 - val_accuracy: 0.6796\n",
      "Epoch 92/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.7500\n",
      "Epoch 00092: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 276s 266ms/step - loss: 0.1445 - accuracy: 0.7500 - val_loss: 0.1863 - val_accuracy: 0.6736\n",
      "Epoch 93/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.7484\n",
      "Epoch 00093: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 275s 265ms/step - loss: 0.1441 - accuracy: 0.7484 - val_loss: 0.1846 - val_accuracy: 0.6768\n",
      "Epoch 94/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.7501\n",
      "Epoch 00094: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 277s 266ms/step - loss: 0.1441 - accuracy: 0.7501 - val_loss: 0.1834 - val_accuracy: 0.6804\n",
      "Epoch 95/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.7494\n",
      "Epoch 00095: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 276s 266ms/step - loss: 0.1434 - accuracy: 0.7494 - val_loss: 0.1825 - val_accuracy: 0.6830\n",
      "Epoch 96/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.7503\n",
      "Epoch 00096: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1432 - accuracy: 0.7503 - val_loss: 0.1811 - val_accuracy: 0.6842\n",
      "Epoch 97/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.7496\n",
      "Epoch 00097: val_accuracy did not improve from 0.68492\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1430 - accuracy: 0.7496 - val_loss: 0.1825 - val_accuracy: 0.6795\n",
      "Epoch 98/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.7528\n",
      "Epoch 00098: val_accuracy improved from 0.68492 to 0.68533, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_98_valloss_0.1805_valacc_0.6853.hdf5\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1428 - accuracy: 0.7528 - val_loss: 0.1805 - val_accuracy: 0.6853\n",
      "Epoch 99/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.7524\n",
      "Epoch 00099: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.1422 - accuracy: 0.7524 - val_loss: 0.1807 - val_accuracy: 0.6840\n",
      "Epoch 100/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.7518\n",
      "Epoch 00100: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.1422 - accuracy: 0.7518 - val_loss: 0.1818 - val_accuracy: 0.6818\n",
      "Epoch 101/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.7517\n",
      "Epoch 00101: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1422 - accuracy: 0.7517 - val_loss: 0.1815 - val_accuracy: 0.6836\n",
      "Epoch 102/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.7549\n",
      "Epoch 00102: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.1414 - accuracy: 0.7549 - val_loss: 0.1840 - val_accuracy: 0.6804\n",
      "Epoch 103/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.7560\n",
      "Epoch 00103: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1413 - accuracy: 0.7560 - val_loss: 0.1850 - val_accuracy: 0.6779\n",
      "Epoch 104/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.7548\n",
      "Epoch 00104: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1407 - accuracy: 0.7548 - val_loss: 0.1820 - val_accuracy: 0.6842\n",
      "Epoch 105/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.7549\n",
      "Epoch 00105: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1410 - accuracy: 0.7549 - val_loss: 0.1809 - val_accuracy: 0.6823\n",
      "Epoch 106/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.7561\n",
      "Epoch 00106: val_accuracy did not improve from 0.68533\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1398 - accuracy: 0.7561 - val_loss: 0.1819 - val_accuracy: 0.6844\n",
      "Epoch 107/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.7545\n",
      "Epoch 00107: val_accuracy improved from 0.68533 to 0.68750, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_107_valloss_0.1805_valacc_0.6875.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1402 - accuracy: 0.7545 - val_loss: 0.1805 - val_accuracy: 0.6875\n",
      "Epoch 108/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.7582\n",
      "Epoch 00108: val_accuracy did not improve from 0.68750\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1393 - accuracy: 0.7582 - val_loss: 0.1832 - val_accuracy: 0.6822\n",
      "Epoch 109/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.7581\n",
      "Epoch 00109: val_accuracy did not improve from 0.68750\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1393 - accuracy: 0.7581 - val_loss: 0.1829 - val_accuracy: 0.6853\n",
      "Epoch 110/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.7587\n",
      "Epoch 00110: val_accuracy did not improve from 0.68750\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1388 - accuracy: 0.7587 - val_loss: 0.1823 - val_accuracy: 0.6818\n",
      "Epoch 111/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.7581\n",
      "Epoch 00111: val_accuracy did not improve from 0.68750\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1390 - accuracy: 0.7581 - val_loss: 0.1815 - val_accuracy: 0.6838\n",
      "Epoch 112/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.7598\n",
      "Epoch 00112: val_accuracy improved from 0.68750 to 0.69117, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_112_valloss_0.1798_valacc_0.6912.hdf5\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1379 - accuracy: 0.7598 - val_loss: 0.1798 - val_accuracy: 0.6912\n",
      "Epoch 113/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.7597\n",
      "Epoch 00113: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1377 - accuracy: 0.7597 - val_loss: 0.1814 - val_accuracy: 0.6859\n",
      "Epoch 114/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.7592\n",
      "Epoch 00114: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1381 - accuracy: 0.7592 - val_loss: 0.1819 - val_accuracy: 0.6856\n",
      "Epoch 115/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.7607\n",
      "Epoch 00115: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1375 - accuracy: 0.7607 - val_loss: 0.1813 - val_accuracy: 0.6872\n",
      "Epoch 116/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.7611\n",
      "Epoch 00116: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1374 - accuracy: 0.7611 - val_loss: 0.1811 - val_accuracy: 0.6871\n",
      "Epoch 117/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.7604\n",
      "Epoch 00117: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1370 - accuracy: 0.7604 - val_loss: 0.1855 - val_accuracy: 0.6837\n",
      "Epoch 118/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.7620\n",
      "Epoch 00118: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1362 - accuracy: 0.7620 - val_loss: 0.1832 - val_accuracy: 0.6859\n",
      "Epoch 119/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.7622\n",
      "Epoch 00119: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1360 - accuracy: 0.7622 - val_loss: 0.1830 - val_accuracy: 0.6859\n",
      "Epoch 120/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.7614\n",
      "Epoch 00120: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1362 - accuracy: 0.7614 - val_loss: 0.1809 - val_accuracy: 0.6879\n",
      "Epoch 121/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.7646\n",
      "Epoch 00121: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1349 - accuracy: 0.7646 - val_loss: 0.1834 - val_accuracy: 0.6844\n",
      "Epoch 122/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.7641\n",
      "Epoch 00122: val_accuracy did not improve from 0.69117\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1351 - accuracy: 0.7641 - val_loss: 0.1840 - val_accuracy: 0.6840\n",
      "Epoch 123/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.7636\n",
      "Epoch 00123: val_accuracy improved from 0.69117 to 0.69158, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_123_valloss_0.1817_valacc_0.6916.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1348 - accuracy: 0.7636 - val_loss: 0.1817 - val_accuracy: 0.6916\n",
      "Epoch 124/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.7665\n",
      "Epoch 00124: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1344 - accuracy: 0.7665 - val_loss: 0.1845 - val_accuracy: 0.6848\n",
      "Epoch 125/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.7662\n",
      "Epoch 00125: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1336 - accuracy: 0.7662 - val_loss: 0.1831 - val_accuracy: 0.6878\n",
      "Epoch 126/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.7667\n",
      "Epoch 00126: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1337 - accuracy: 0.7667 - val_loss: 0.1859 - val_accuracy: 0.6804\n",
      "Epoch 127/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.7671\n",
      "Epoch 00127: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1334 - accuracy: 0.7671 - val_loss: 0.1831 - val_accuracy: 0.6863\n",
      "Epoch 128/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.7677\n",
      "Epoch 00128: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1328 - accuracy: 0.7677 - val_loss: 0.1863 - val_accuracy: 0.6842\n",
      "Epoch 129/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.7661\n",
      "Epoch 00129: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1329 - accuracy: 0.7661 - val_loss: 0.1832 - val_accuracy: 0.6891\n",
      "Epoch 130/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.7674\n",
      "Epoch 00130: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1326 - accuracy: 0.7674 - val_loss: 0.1827 - val_accuracy: 0.6901\n",
      "Epoch 131/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.7679\n",
      "Epoch 00131: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1326 - accuracy: 0.7679 - val_loss: 0.1845 - val_accuracy: 0.6861\n",
      "Epoch 132/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.7702\n",
      "Epoch 00132: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1308 - accuracy: 0.7702 - val_loss: 0.1837 - val_accuracy: 0.6891\n",
      "Epoch 133/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.7694\n",
      "Epoch 00133: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1317 - accuracy: 0.7694 - val_loss: 0.1837 - val_accuracy: 0.6880\n",
      "Epoch 134/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.7726\n",
      "Epoch 00134: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1311 - accuracy: 0.7726 - val_loss: 0.1820 - val_accuracy: 0.6901\n",
      "Epoch 135/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.7712\n",
      "Epoch 00135: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1312 - accuracy: 0.7712 - val_loss: 0.1829 - val_accuracy: 0.6857\n",
      "Epoch 136/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.7717\n",
      "Epoch 00136: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1311 - accuracy: 0.7717 - val_loss: 0.1840 - val_accuracy: 0.6872\n",
      "Epoch 137/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.7708\n",
      "Epoch 00137: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1305 - accuracy: 0.7708 - val_loss: 0.1839 - val_accuracy: 0.6868\n",
      "Epoch 138/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.7728\n",
      "Epoch 00138: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1304 - accuracy: 0.7728 - val_loss: 0.1824 - val_accuracy: 0.6889\n",
      "Epoch 139/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.7708\n",
      "Epoch 00139: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1303 - accuracy: 0.7708 - val_loss: 0.1830 - val_accuracy: 0.6904\n",
      "Epoch 140/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.7725\n",
      "Epoch 00140: val_accuracy did not improve from 0.69158\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1298 - accuracy: 0.7725 - val_loss: 0.1857 - val_accuracy: 0.6874\n",
      "Epoch 141/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.7735\n",
      "Epoch 00141: val_accuracy improved from 0.69158 to 0.69212, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_141_valloss_0.1827_valacc_0.6921.hdf5\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1294 - accuracy: 0.7735 - val_loss: 0.1827 - val_accuracy: 0.6921\n",
      "Epoch 142/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.7743\n",
      "Epoch 00142: val_accuracy did not improve from 0.69212\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1285 - accuracy: 0.7743 - val_loss: 0.1841 - val_accuracy: 0.6921\n",
      "Epoch 143/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.7746\n",
      "Epoch 00143: val_accuracy improved from 0.69212 to 0.69280, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_143_valloss_0.1829_valacc_0.6928.hdf5\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1286 - accuracy: 0.7746 - val_loss: 0.1829 - val_accuracy: 0.6928\n",
      "Epoch 144/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.7765\n",
      "Epoch 00144: val_accuracy did not improve from 0.69280\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1280 - accuracy: 0.7765 - val_loss: 0.1865 - val_accuracy: 0.6904\n",
      "Epoch 145/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.7755\n",
      "Epoch 00145: val_accuracy did not improve from 0.69280\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1277 - accuracy: 0.7755 - val_loss: 0.1846 - val_accuracy: 0.6885\n",
      "Epoch 146/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.7762\n",
      "Epoch 00146: val_accuracy did not improve from 0.69280\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1279 - accuracy: 0.7762 - val_loss: 0.1837 - val_accuracy: 0.6914\n",
      "Epoch 147/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.7767\n",
      "Epoch 00147: val_accuracy improved from 0.69280 to 0.69307, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_147_valloss_0.1835_valacc_0.6931.hdf5\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1273 - accuracy: 0.7767 - val_loss: 0.1835 - val_accuracy: 0.6931\n",
      "Epoch 148/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.7784\n",
      "Epoch 00148: val_accuracy did not improve from 0.69307\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1266 - accuracy: 0.7784 - val_loss: 0.1834 - val_accuracy: 0.6906\n",
      "Epoch 149/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.7783\n",
      "Epoch 00149: val_accuracy improved from 0.69307 to 0.69375, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_149_valloss_0.1839_valacc_0.6938.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1263 - accuracy: 0.7783 - val_loss: 0.1839 - val_accuracy: 0.6938\n",
      "Epoch 150/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.7774\n",
      "Epoch 00150: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1262 - accuracy: 0.7774 - val_loss: 0.1849 - val_accuracy: 0.6933\n",
      "Epoch 151/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.7790\n",
      "Epoch 00151: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1256 - accuracy: 0.7790 - val_loss: 0.1846 - val_accuracy: 0.6913\n",
      "Epoch 152/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.7789\n",
      "Epoch 00152: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1258 - accuracy: 0.7789 - val_loss: 0.1859 - val_accuracy: 0.6901\n",
      "Epoch 153/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.7788\n",
      "Epoch 00153: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1255 - accuracy: 0.7788 - val_loss: 0.1843 - val_accuracy: 0.6914\n",
      "Epoch 154/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.7809\n",
      "Epoch 00154: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1252 - accuracy: 0.7809 - val_loss: 0.1848 - val_accuracy: 0.6912\n",
      "Epoch 155/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.7795\n",
      "Epoch 00155: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1254 - accuracy: 0.7795 - val_loss: 0.1866 - val_accuracy: 0.6924\n",
      "Epoch 156/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.7808\n",
      "Epoch 00156: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1249 - accuracy: 0.7808 - val_loss: 0.1869 - val_accuracy: 0.6914\n",
      "Epoch 157/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.7817\n",
      "Epoch 00157: val_accuracy did not improve from 0.69375\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1239 - accuracy: 0.7817 - val_loss: 0.1854 - val_accuracy: 0.6920\n",
      "Epoch 158/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.7806\n",
      "Epoch 00158: val_accuracy improved from 0.69375 to 0.69511, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_158_valloss_0.1852_valacc_0.6951.hdf5\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1243 - accuracy: 0.7806 - val_loss: 0.1852 - val_accuracy: 0.6951\n",
      "Epoch 159/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.7821\n",
      "Epoch 00159: val_accuracy improved from 0.69511 to 0.69538, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_159_valloss_0.1828_valacc_0.6954.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1239 - accuracy: 0.7821 - val_loss: 0.1828 - val_accuracy: 0.6954\n",
      "Epoch 160/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.7842\n",
      "Epoch 00160: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1234 - accuracy: 0.7842 - val_loss: 0.1868 - val_accuracy: 0.6895\n",
      "Epoch 161/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.7828\n",
      "Epoch 00161: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1236 - accuracy: 0.7828 - val_loss: 0.1826 - val_accuracy: 0.6952\n",
      "Epoch 162/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.7838\n",
      "Epoch 00162: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1231 - accuracy: 0.7838 - val_loss: 0.1883 - val_accuracy: 0.6890\n",
      "Epoch 163/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.7849\n",
      "Epoch 00163: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1226 - accuracy: 0.7849 - val_loss: 0.1868 - val_accuracy: 0.6924\n",
      "Epoch 164/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.7863\n",
      "Epoch 00164: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1214 - accuracy: 0.7863 - val_loss: 0.1862 - val_accuracy: 0.6909\n",
      "Epoch 165/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.7844\n",
      "Epoch 00165: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1222 - accuracy: 0.7844 - val_loss: 0.1855 - val_accuracy: 0.6931\n",
      "Epoch 166/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.7850\n",
      "Epoch 00166: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1221 - accuracy: 0.7850 - val_loss: 0.1865 - val_accuracy: 0.6914\n",
      "Epoch 167/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.7853\n",
      "Epoch 00167: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1217 - accuracy: 0.7853 - val_loss: 0.1870 - val_accuracy: 0.6914\n",
      "Epoch 168/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.7855\n",
      "Epoch 00168: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1216 - accuracy: 0.7855 - val_loss: 0.1853 - val_accuracy: 0.6924\n",
      "Epoch 169/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.7870\n",
      "Epoch 00169: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1208 - accuracy: 0.7870 - val_loss: 0.1849 - val_accuracy: 0.6933\n",
      "Epoch 170/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.7879\n",
      "Epoch 00170: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1207 - accuracy: 0.7879 - val_loss: 0.1858 - val_accuracy: 0.6923\n",
      "Epoch 171/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.7887\n",
      "Epoch 00171: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1204 - accuracy: 0.7887 - val_loss: 0.1887 - val_accuracy: 0.6897\n",
      "Epoch 172/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.7881\n",
      "Epoch 00172: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1204 - accuracy: 0.7881 - val_loss: 0.1867 - val_accuracy: 0.6936\n",
      "Epoch 173/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.7874\n",
      "Epoch 00173: val_accuracy did not improve from 0.69538\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1206 - accuracy: 0.7874 - val_loss: 0.1842 - val_accuracy: 0.6950\n",
      "Epoch 174/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.7890\n",
      "Epoch 00174: val_accuracy improved from 0.69538 to 0.69674, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_174_valloss_0.1844_valacc_0.6967.hdf5\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1195 - accuracy: 0.7890 - val_loss: 0.1844 - val_accuracy: 0.6967\n",
      "Epoch 175/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.7878\n",
      "Epoch 00175: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1200 - accuracy: 0.7878 - val_loss: 0.1845 - val_accuracy: 0.6962\n",
      "Epoch 176/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.7900\n",
      "Epoch 00176: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1195 - accuracy: 0.7900 - val_loss: 0.1862 - val_accuracy: 0.6939\n",
      "Epoch 177/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.7913\n",
      "Epoch 00177: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1187 - accuracy: 0.7913 - val_loss: 0.1853 - val_accuracy: 0.6946\n",
      "Epoch 178/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.7908\n",
      "Epoch 00178: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1180 - accuracy: 0.7908 - val_loss: 0.1888 - val_accuracy: 0.6901\n",
      "Epoch 179/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.7906\n",
      "Epoch 00179: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1183 - accuracy: 0.7906 - val_loss: 0.1882 - val_accuracy: 0.6946\n",
      "Epoch 180/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.7902\n",
      "Epoch 00180: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1183 - accuracy: 0.7902 - val_loss: 0.1881 - val_accuracy: 0.6928\n",
      "Epoch 181/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.7937\n",
      "Epoch 00181: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1175 - accuracy: 0.7937 - val_loss: 0.1882 - val_accuracy: 0.6917\n",
      "Epoch 182/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.7923\n",
      "Epoch 00182: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1177 - accuracy: 0.7923 - val_loss: 0.1872 - val_accuracy: 0.6951\n",
      "Epoch 183/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.7931\n",
      "Epoch 00183: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1172 - accuracy: 0.7931 - val_loss: 0.1878 - val_accuracy: 0.6955\n",
      "Epoch 184/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.7934\n",
      "Epoch 00184: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1171 - accuracy: 0.7934 - val_loss: 0.1868 - val_accuracy: 0.6939\n",
      "Epoch 185/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.7939\n",
      "Epoch 00185: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1170 - accuracy: 0.7939 - val_loss: 0.1887 - val_accuracy: 0.6924\n",
      "Epoch 186/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.7951\n",
      "Epoch 00186: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1164 - accuracy: 0.7951 - val_loss: 0.1875 - val_accuracy: 0.6951\n",
      "Epoch 187/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.7939\n",
      "Epoch 00187: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1162 - accuracy: 0.7939 - val_loss: 0.1882 - val_accuracy: 0.6962\n",
      "Epoch 188/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.7947\n",
      "Epoch 00188: val_accuracy did not improve from 0.69674\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1157 - accuracy: 0.7947 - val_loss: 0.1893 - val_accuracy: 0.6967\n",
      "Epoch 189/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.7964\n",
      "Epoch 00189: val_accuracy improved from 0.69674 to 0.69687, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_189_valloss_0.1887_valacc_0.6969.hdf5\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1149 - accuracy: 0.7964 - val_loss: 0.1887 - val_accuracy: 0.6969\n",
      "Epoch 190/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.7959\n",
      "Epoch 00190: val_accuracy did not improve from 0.69687\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1153 - accuracy: 0.7959 - val_loss: 0.1878 - val_accuracy: 0.6955\n",
      "Epoch 191/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.7957\n",
      "Epoch 00191: val_accuracy improved from 0.69687 to 0.70177, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_191_valloss_0.1867_valacc_0.7018.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1148 - accuracy: 0.7957 - val_loss: 0.1867 - val_accuracy: 0.7018\n",
      "Epoch 192/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.7964\n",
      "Epoch 00192: val_accuracy did not improve from 0.70177\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1147 - accuracy: 0.7964 - val_loss: 0.1907 - val_accuracy: 0.6939\n",
      "Epoch 193/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.7980\n",
      "Epoch 00193: val_accuracy improved from 0.70177 to 0.70299, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_193_valloss_0.1864_valacc_0.7030.hdf5\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1144 - accuracy: 0.7980 - val_loss: 0.1864 - val_accuracy: 0.7030\n",
      "Epoch 194/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.7976\n",
      "Epoch 00194: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1142 - accuracy: 0.7976 - val_loss: 0.1895 - val_accuracy: 0.6963\n",
      "Epoch 195/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.7998\n",
      "Epoch 00195: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1145 - accuracy: 0.7998 - val_loss: 0.1910 - val_accuracy: 0.6942\n",
      "Epoch 196/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.7984\n",
      "Epoch 00196: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1135 - accuracy: 0.7984 - val_loss: 0.1887 - val_accuracy: 0.7007\n",
      "Epoch 197/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.7994\n",
      "Epoch 00197: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1139 - accuracy: 0.7994 - val_loss: 0.1894 - val_accuracy: 0.6980\n",
      "Epoch 198/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.8002\n",
      "Epoch 00198: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1132 - accuracy: 0.8002 - val_loss: 0.1899 - val_accuracy: 0.6962\n",
      "Epoch 199/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.7989\n",
      "Epoch 00199: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1132 - accuracy: 0.7989 - val_loss: 0.1879 - val_accuracy: 0.6988\n",
      "Epoch 200/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.8003\n",
      "Epoch 00200: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 282s 271ms/step - loss: 0.1129 - accuracy: 0.8003 - val_loss: 0.1901 - val_accuracy: 0.6988\n",
      "Epoch 201/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.8026\n",
      "Epoch 00201: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1118 - accuracy: 0.8026 - val_loss: 0.1905 - val_accuracy: 0.6980\n",
      "Epoch 202/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.8016\n",
      "Epoch 00202: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1119 - accuracy: 0.8016 - val_loss: 0.1901 - val_accuracy: 0.6958\n",
      "Epoch 203/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.8015\n",
      "Epoch 00203: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1118 - accuracy: 0.8015 - val_loss: 0.1913 - val_accuracy: 0.6957\n",
      "Epoch 204/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.8011\n",
      "Epoch 00204: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1122 - accuracy: 0.8011 - val_loss: 0.1900 - val_accuracy: 0.6992\n",
      "Epoch 205/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.8016\n",
      "Epoch 00205: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1112 - accuracy: 0.8016 - val_loss: 0.1889 - val_accuracy: 0.7024\n",
      "Epoch 206/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.8031\n",
      "Epoch 00206: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1117 - accuracy: 0.8031 - val_loss: 0.1915 - val_accuracy: 0.6957\n",
      "Epoch 207/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.8048\n",
      "Epoch 00207: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1108 - accuracy: 0.8048 - val_loss: 0.1898 - val_accuracy: 0.6980\n",
      "Epoch 208/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.8046\n",
      "Epoch 00208: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1104 - accuracy: 0.8046 - val_loss: 0.1925 - val_accuracy: 0.6955\n",
      "Epoch 209/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.8040\n",
      "Epoch 00209: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1102 - accuracy: 0.8040 - val_loss: 0.1941 - val_accuracy: 0.6950\n",
      "Epoch 210/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.8042\n",
      "Epoch 00210: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1105 - accuracy: 0.8042 - val_loss: 0.1937 - val_accuracy: 0.6929\n",
      "Epoch 211/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.8053\n",
      "Epoch 00211: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1097 - accuracy: 0.8053 - val_loss: 0.1911 - val_accuracy: 0.6982\n",
      "Epoch 212/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.8048\n",
      "Epoch 00212: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1094 - accuracy: 0.8048 - val_loss: 0.1943 - val_accuracy: 0.6940\n",
      "Epoch 213/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.8046\n",
      "Epoch 00213: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 282s 272ms/step - loss: 0.1095 - accuracy: 0.8046 - val_loss: 0.1900 - val_accuracy: 0.6970\n",
      "Epoch 214/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.8065\n",
      "Epoch 00214: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1090 - accuracy: 0.8065 - val_loss: 0.1919 - val_accuracy: 0.6963\n",
      "Epoch 215/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.8055\n",
      "Epoch 00215: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1096 - accuracy: 0.8055 - val_loss: 0.1906 - val_accuracy: 0.6957\n",
      "Epoch 216/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.8070\n",
      "Epoch 00216: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1085 - accuracy: 0.8070 - val_loss: 0.1905 - val_accuracy: 0.6986\n",
      "Epoch 217/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.8072\n",
      "Epoch 00217: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1090 - accuracy: 0.8072 - val_loss: 0.1940 - val_accuracy: 0.6950\n",
      "Epoch 218/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.8084\n",
      "Epoch 00218: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1077 - accuracy: 0.8084 - val_loss: 0.1927 - val_accuracy: 0.6965\n",
      "Epoch 219/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.8083\n",
      "Epoch 00219: val_accuracy did not improve from 0.70299\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1079 - accuracy: 0.8083 - val_loss: 0.1893 - val_accuracy: 0.7018\n",
      "Epoch 220/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.8065\n",
      "Epoch 00220: val_accuracy improved from 0.70299 to 0.70326, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_220_valloss_0.1899_valacc_0.7033.hdf5\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1080 - accuracy: 0.8065 - val_loss: 0.1899 - val_accuracy: 0.7033\n",
      "Epoch 221/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.8086\n",
      "Epoch 00221: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1072 - accuracy: 0.8086 - val_loss: 0.1916 - val_accuracy: 0.6986\n",
      "Epoch 222/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.8090\n",
      "Epoch 00222: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 281s 271ms/step - loss: 0.1069 - accuracy: 0.8090 - val_loss: 0.1933 - val_accuracy: 0.7024\n",
      "Epoch 223/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.8091\n",
      "Epoch 00223: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1069 - accuracy: 0.8091 - val_loss: 0.1895 - val_accuracy: 0.7011\n",
      "Epoch 224/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.8094\n",
      "Epoch 00224: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 283s 272ms/step - loss: 0.1068 - accuracy: 0.8094 - val_loss: 0.1970 - val_accuracy: 0.6936\n",
      "Epoch 225/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.8108\n",
      "Epoch 00225: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1063 - accuracy: 0.8108 - val_loss: 0.1925 - val_accuracy: 0.6986\n",
      "Epoch 226/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.8124\n",
      "Epoch 00226: val_accuracy did not improve from 0.70326\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1060 - accuracy: 0.8124 - val_loss: 0.1926 - val_accuracy: 0.6976\n",
      "Epoch 227/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.8113\n",
      "Epoch 00227: val_accuracy improved from 0.70326 to 0.70367, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_227_valloss_0.1917_valacc_0.7037.hdf5\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1058 - accuracy: 0.8113 - val_loss: 0.1917 - val_accuracy: 0.7037\n",
      "Epoch 228/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.8108\n",
      "Epoch 00228: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1053 - accuracy: 0.8108 - val_loss: 0.1943 - val_accuracy: 0.7008\n",
      "Epoch 229/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.8129\n",
      "Epoch 00229: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1055 - accuracy: 0.8129 - val_loss: 0.1957 - val_accuracy: 0.6969\n",
      "Epoch 230/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.8122\n",
      "Epoch 00230: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1054 - accuracy: 0.8122 - val_loss: 0.1950 - val_accuracy: 0.6978\n",
      "Epoch 231/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.8125\n",
      "Epoch 00231: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1049 - accuracy: 0.8125 - val_loss: 0.1943 - val_accuracy: 0.6988\n",
      "Epoch 232/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.8137\n",
      "Epoch 00232: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1048 - accuracy: 0.8137 - val_loss: 0.1949 - val_accuracy: 0.6970\n",
      "Epoch 233/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.8155\n",
      "Epoch 00233: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1042 - accuracy: 0.8155 - val_loss: 0.1944 - val_accuracy: 0.6999\n",
      "Epoch 234/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.8142\n",
      "Epoch 00234: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 283s 272ms/step - loss: 0.1041 - accuracy: 0.8142 - val_loss: 0.1945 - val_accuracy: 0.7000\n",
      "Epoch 235/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.8145\n",
      "Epoch 00235: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1038 - accuracy: 0.8145 - val_loss: 0.1941 - val_accuracy: 0.7004\n",
      "Epoch 236/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.8162\n",
      "Epoch 00236: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1035 - accuracy: 0.8162 - val_loss: 0.1952 - val_accuracy: 0.6961\n",
      "Epoch 237/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.8150\n",
      "Epoch 00237: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1038 - accuracy: 0.8150 - val_loss: 0.1945 - val_accuracy: 0.7011\n",
      "Epoch 238/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.8171\n",
      "Epoch 00238: val_accuracy did not improve from 0.70367\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1027 - accuracy: 0.8171 - val_loss: 0.1984 - val_accuracy: 0.6978\n",
      "Epoch 239/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.8154\n",
      "Epoch 00239: val_accuracy improved from 0.70367 to 0.70394, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_239_valloss_0.1944_valacc_0.7039.hdf5\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1030 - accuracy: 0.8154 - val_loss: 0.1944 - val_accuracy: 0.7039\n",
      "Epoch 240/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.8170\n",
      "Epoch 00240: val_accuracy did not improve from 0.70394\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1029 - accuracy: 0.8170 - val_loss: 0.1959 - val_accuracy: 0.7001\n",
      "Epoch 241/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.8153\n",
      "Epoch 00241: val_accuracy did not improve from 0.70394\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1028 - accuracy: 0.8153 - val_loss: 0.1945 - val_accuracy: 0.6996\n",
      "Epoch 242/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.8165\n",
      "Epoch 00242: val_accuracy did not improve from 0.70394\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1029 - accuracy: 0.8165 - val_loss: 0.1958 - val_accuracy: 0.6984\n",
      "Epoch 243/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.8178\n",
      "Epoch 00243: val_accuracy did not improve from 0.70394\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1018 - accuracy: 0.8178 - val_loss: 0.1955 - val_accuracy: 0.7001\n",
      "Epoch 244/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.8184\n",
      "Epoch 00244: val_accuracy improved from 0.70394 to 0.70489, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_244_valloss_0.1941_valacc_0.7049.hdf5\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1013 - accuracy: 0.8184 - val_loss: 0.1941 - val_accuracy: 0.7049\n",
      "Epoch 245/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.8185\n",
      "Epoch 00245: val_accuracy did not improve from 0.70489\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1017 - accuracy: 0.8185 - val_loss: 0.1995 - val_accuracy: 0.6952\n",
      "Epoch 246/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.8174\n",
      "Epoch 00246: val_accuracy did not improve from 0.70489\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1021 - accuracy: 0.8174 - val_loss: 0.1970 - val_accuracy: 0.6957\n",
      "Epoch 247/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.8184\n",
      "Epoch 00247: val_accuracy did not improve from 0.70489\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.1010 - accuracy: 0.8184 - val_loss: 0.1931 - val_accuracy: 0.7019\n",
      "Epoch 248/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.8186\n",
      "Epoch 00248: val_accuracy did not improve from 0.70489\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.1010 - accuracy: 0.8186 - val_loss: 0.1969 - val_accuracy: 0.6995\n",
      "Epoch 249/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.8190\n",
      "Epoch 00249: val_accuracy did not improve from 0.70489\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1012 - accuracy: 0.8190 - val_loss: 0.1979 - val_accuracy: 0.6978\n",
      "Epoch 250/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.8177\n",
      "Epoch 00250: val_accuracy improved from 0.70489 to 0.70530, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_250_valloss_0.1938_valacc_0.7053.hdf5\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.1012 - accuracy: 0.8177 - val_loss: 0.1938 - val_accuracy: 0.7053\n",
      "Epoch 251/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.8199\n",
      "Epoch 00251: val_accuracy did not improve from 0.70530\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1003 - accuracy: 0.8199 - val_loss: 0.1964 - val_accuracy: 0.7005\n",
      "Epoch 252/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.8208\n",
      "Epoch 00252: val_accuracy did not improve from 0.70530\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0999 - accuracy: 0.8208 - val_loss: 0.1957 - val_accuracy: 0.7038\n",
      "Epoch 253/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.8205\n",
      "Epoch 00253: val_accuracy did not improve from 0.70530\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.1003 - accuracy: 0.8205 - val_loss: 0.2006 - val_accuracy: 0.6989\n",
      "Epoch 254/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.8210\n",
      "Epoch 00254: val_accuracy did not improve from 0.70530\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0995 - accuracy: 0.8210 - val_loss: 0.1973 - val_accuracy: 0.7018\n",
      "Epoch 255/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.8202\n",
      "Epoch 00255: val_accuracy improved from 0.70530 to 0.70652, saving model to /home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_255_valloss_0.1925_valacc_0.7065.hdf5\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.1000 - accuracy: 0.8202 - val_loss: 0.1925 - val_accuracy: 0.7065\n",
      "Epoch 256/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.8220\n",
      "Epoch 00256: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0991 - accuracy: 0.8220 - val_loss: 0.1990 - val_accuracy: 0.7001\n",
      "Epoch 257/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.8218\n",
      "Epoch 00257: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0992 - accuracy: 0.8218 - val_loss: 0.1976 - val_accuracy: 0.7035\n",
      "Epoch 258/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.8207\n",
      "Epoch 00258: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0988 - accuracy: 0.8207 - val_loss: 0.2004 - val_accuracy: 0.6996\n",
      "Epoch 259/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.8237\n",
      "Epoch 00259: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0991 - accuracy: 0.8237 - val_loss: 0.1983 - val_accuracy: 0.7041\n",
      "Epoch 260/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.8247\n",
      "Epoch 00260: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0983 - accuracy: 0.8247 - val_loss: 0.1979 - val_accuracy: 0.7011\n",
      "Epoch 261/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.8234\n",
      "Epoch 00261: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0981 - accuracy: 0.8234 - val_loss: 0.1963 - val_accuracy: 0.7015\n",
      "Epoch 262/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.8232\n",
      "Epoch 00262: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0982 - accuracy: 0.8232 - val_loss: 0.1987 - val_accuracy: 0.7024\n",
      "Epoch 263/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.8257\n",
      "Epoch 00263: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.0977 - accuracy: 0.8257 - val_loss: 0.1983 - val_accuracy: 0.7041\n",
      "Epoch 264/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.8245\n",
      "Epoch 00264: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.0972 - accuracy: 0.8245 - val_loss: 0.1981 - val_accuracy: 0.7031\n",
      "Epoch 265/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.8260\n",
      "Epoch 00265: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0974 - accuracy: 0.8260 - val_loss: 0.2014 - val_accuracy: 0.7011\n",
      "Epoch 266/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.8265\n",
      "Epoch 00266: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0967 - accuracy: 0.8265 - val_loss: 0.1969 - val_accuracy: 0.7041\n",
      "Epoch 267/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.8266\n",
      "Epoch 00267: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0971 - accuracy: 0.8266 - val_loss: 0.2031 - val_accuracy: 0.7010\n",
      "Epoch 268/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.8273\n",
      "Epoch 00268: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0960 - accuracy: 0.8273 - val_loss: 0.1998 - val_accuracy: 0.7023\n",
      "Epoch 269/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.8288\n",
      "Epoch 00269: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0961 - accuracy: 0.8288 - val_loss: 0.2002 - val_accuracy: 0.7020\n",
      "Epoch 270/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.8271\n",
      "Epoch 00270: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 281s 270ms/step - loss: 0.0961 - accuracy: 0.8271 - val_loss: 0.1996 - val_accuracy: 0.7022\n",
      "Epoch 271/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.8277\n",
      "Epoch 00271: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0962 - accuracy: 0.8277 - val_loss: 0.2047 - val_accuracy: 0.6974\n",
      "Epoch 272/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.8281\n",
      "Epoch 00272: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0956 - accuracy: 0.8281 - val_loss: 0.1996 - val_accuracy: 0.7001\n",
      "Epoch 273/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.8282\n",
      "Epoch 00273: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 279s 269ms/step - loss: 0.0955 - accuracy: 0.8282 - val_loss: 0.2001 - val_accuracy: 0.7052\n",
      "Epoch 274/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.8289\n",
      "Epoch 00274: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0951 - accuracy: 0.8289 - val_loss: 0.2001 - val_accuracy: 0.7045\n",
      "Epoch 275/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.8279\n",
      "Epoch 00275: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0956 - accuracy: 0.8279 - val_loss: 0.2039 - val_accuracy: 0.6989\n",
      "Epoch 276/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.8307\n",
      "Epoch 00276: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 270ms/step - loss: 0.0942 - accuracy: 0.8307 - val_loss: 0.2034 - val_accuracy: 0.7020\n",
      "Epoch 277/400\n",
      "1039/1039 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.8290\n",
      "Epoch 00277: val_accuracy did not improve from 0.70652\n",
      "1039/1039 [==============================] - 280s 269ms/step - loss: 0.0949 - accuracy: 0.8290 - val_loss: 0.2000 - val_accuracy: 0.7053\n",
      "Epoch 278/400\n",
      " 893/1039 [========================>.....] - ETA: 38s - loss: 0.0939 - accuracy: 0.8309"
     ]
    }
   ],
   "source": [
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:0.7068\n",
      "/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210323_120335/STAR2000/epoch_278_valloss_0.1995_valacc_0.7068.hdf5\n"
     ]
    }
   ],
   "source": [
    "# fit_result_path2 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210310_095616/STAR2000'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 100, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 100, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 50, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 50, 128)           443136    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 615,928\n",
      "Trainable params: 615,480\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5mrt# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2000_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2003_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_STAR2006_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_HICEAS2002_orig.npz'))\n",
    "# fea_temp = np.load(os.path.join(feature_path, 'oswald_PICEAS2005_orig.npz'))\n",
    "    \n",
    "fea_train = fea_temp['feas_orig']\n",
    "label_train_list = fea_temp['labels_orig']\n",
    "del fea_temp\n",
    "\n",
    "fea_train = fea_train[:,:100,:]\n",
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# testing the training data\n",
    "label_train_pred = model.predict(fea_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_train[:label_train_pred.shape[0]], np.argmax(label_train_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## top k accuracy score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "top_k = []\n",
    "for kk in range(1, num_species+1):\n",
    "    print('k='+str(kk)+':  ')\n",
    "    this_acc = top_k_accuracy_score(label_train[:label_train_pred.shape[0]], label_train_pred, k=kk, labels=list(range(num_species)))\n",
    "    print(this_acc)\n",
    "    top_k.append(this_acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.bar(list(range(1, num_species+1)), top_k)\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data cleaning. Remove those not trained well\n",
    "np.savez(os.path.join(feature_path, 'train_oswald_no_'+ee+'_label_correct.npz'), label_train=label_train, label_train_pred=label_train_pred )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train = label_train[:label_train_pred.shape[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_train_pred[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "['BD', 'CD', 'STR', 'SPT', 'SPIN', 'PLT', 'RT', 'FKW']\n",
      "\n",
      "[[  34   14    8   38    1    5   16    5]\n",
      " [ 502 1700  480  701  124  318  126   13]\n",
      " [ 224  207  218  312   40  104   21   14]\n",
      " [  64  279   15  396   10   66   12    3]\n",
      " [ 194   26   24  165   69   10    3    0]\n",
      " [   0    0    0    1    0   25    2    3]\n",
      " [  16    1    8    6    3    3   31    8]\n",
      " [   0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[[0.28 0.12 0.07 0.31 0.01 0.04 0.13 0.04]\n",
      " [0.13 0.43 0.12 0.18 0.03 0.08 0.03 0.  ]\n",
      " [0.2  0.18 0.19 0.27 0.04 0.09 0.02 0.01]\n",
      " [0.08 0.33 0.02 0.47 0.01 0.08 0.01 0.  ]\n",
      " [0.4  0.05 0.05 0.34 0.14 0.02 0.01 0.  ]\n",
      " [0.   0.   0.   0.03 0.   0.81 0.06 0.1 ]\n",
      " [0.21 0.01 0.11 0.08 0.04 0.04 0.41 0.11]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5dc2bcfa90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5dc2bcf5e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# >>> target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(label_test, np.argmax(label_pred, axis=1), target_names=list(range(num_species))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top k accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1:  \n",
      "0.370875824835033\n",
      "k=2:  \n",
      "0.5574385122975405\n",
      "k=3:  \n",
      "0.6774145170965807\n",
      "k=4:  \n",
      "0.7936412717456509\n",
      "k=5:  \n",
      "0.8797240551889622\n",
      "k=6:  \n",
      "0.9353629274145171\n",
      "k=7:  \n",
      "0.974505098980204\n",
      "k=8:  \n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys587/miniconda3/envs/whistle_classifier/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1693: UndefinedMetricWarning: 'k' (8) greater than or equal to 'n_classes' (8) will result in a perfect score and is therefore meaningless.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "top_k = []\n",
    "for kk in range(1, num_species+1):\n",
    "    print('k='+str(kk)+':  ')\n",
    "    this_acc = top_k_accuracy_score(label_test, label_pred, k=kk, labels=list(range(num_species)))\n",
    "    print(this_acc)\n",
    "    top_k.append(this_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.bar(list(range(1, num_species+1)), top_k)\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average_precision_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "average_precision_score(to_categorical(label_test, num_classes=8), label_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "Counter(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAR2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = deployment[1]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_conv_lstm(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAR2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = deployment[2]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_conv_lstm(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HICEAS2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = deployment[3]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_conv_lstm(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICES2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = deployment[4]\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'train_oswald_no_'+ee+'.npz'))\n",
    "fea_train = fea_temp['fea_train']\n",
    "label_train_list = fea_temp['label_train']\n",
    "del fea_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "fea_temp = np.load(os.path.join(feature_path, 'oswald_'+ee+'_orig.npz'))\n",
    "fea_test = fea_temp['feas_orig']\n",
    "label_test_list = fea_temp['labels_orig']\n",
    "\n",
    "fea_test = fea_test[:,:100,:]\n",
    "label_test = np.zeros(len(label_test_list))\n",
    "for ii in range(len(label_test_list)):\n",
    "    label_test[ii] = species_dict[label_test_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = fea_train[:,:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros(len(label_train_list))\n",
    "for ii in range(len(label_train_list)):\n",
    "    label_train[ii] = species_dict[label_train_list[ii]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_path2 = os.path.join(fit_result_path1, ee)\n",
    "if not os.path.exists(fit_result_path2):\n",
    "    makedirs(fit_result_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature train shape: '+str(fea_train.shape))\n",
    "print('feature test shape: '+str(fea_test.shape))\n",
    "print('label train shape: '+str(label_train.shape))\n",
    "print('label test shape: '+str(label_test.shape))\n",
    "\n",
    "dim_time = fea_train.shape[1]\n",
    "dim_freq = fea_train.shape[2]\n",
    "print('dim_time: '+str(dim_time))\n",
    "print('dim_freq: '+str(dim_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features & labels\n",
    "fea_train, label_train = shuffle(fea_train, label_train, random_state=0)\n",
    "fea_test, label_test = shuffle(fea_test, label_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(label_train), y=label_train)\n",
    "\n",
    "class_weights = dict()\n",
    "for ii in range(num_species):\n",
    "    class_weights[ii] = weights[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = np.expand_dims(fea_train, axis=3)\n",
    "fea_test = np.expand_dims(fea_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_validate, label_train, label_validate = train_test_split(fea_train, label_train, test_size=0.10, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(fea_train, label_train, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_train\n",
    "validate_generator = DataGenerator(fea_validate, label_validate, batch_size=batch_size, num_classes=num_species)\n",
    "del fea_validate\n",
    "\n",
    "# test_generator = DataGenerator(fea_test, label_test, batch_size=batch_size, num_classes=num_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = model_cnn14_attention_multi(dim_time, dim_freq, num_species, model_type='feature_level_attention', conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# model = model_cnn14_spp(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "model = model_conv_lstm(dim_time, dim_freq, num_species, conv_dim=conv_dim, pool_size=pool_size, pool_stride=pool_stride, hidden_units=hidden_units, l2_regu=l2_regu, drop_rate=drop_rate)\n",
    "# loss = CategoricalCrossentropy()\n",
    "loss = binary_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# With classes\n",
    "# history = model.fit(fea_train, to_categorical(label_train), class_weight=class_weights, validation_split=0.3, batch_size=batch_size, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience), ModelCheckpoint(filepath=os.path.join(fit_result_path, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path1), ModelCheckpoint(filepath=os.path.join(fit_result_path1, '{epoch:02d}-{val_loss:.4f}.hdf5'), verbose=1, monitor=\"val_loss\", save_best_only=True)])\n",
    "# history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_loss', mode='min', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])\n",
    "history = model.fit(train_generator, validation_data=validate_generator, class_weight=class_weights, epochs=num_epoch, callbacks=[EarlyStopping(patience=num_patience, monitor='val_accuracy', mode='max', verbose=1), TensorBoard(log_dir=fit_result_path2), ModelCheckpoint(filepath=os.path.join(fit_result_path2, 'epoch_{epoch:02d}_valloss_{val_loss:.4f}_valacc_{val_accuracy:.4f}.hdf5' ), verbose=1, monitor=\"val_accuracy\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_result_path1 = '/home/ys587/__Data/__whistle/__whislte_30_species/__fit_result_species/20210210_224527'\n",
    "the_best_model, _ = find_best_model(fit_result_path2, purge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(the_best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model.predict(fea_test)\n",
    "# label_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200, precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(label_test, np.argmax(label_pred, axis=1), labels=species_id)\n",
    "\n",
    "print(species_list)\n",
    "print('')\n",
    "print(cm)\n",
    "print('')\n",
    "\n",
    "cm2 = cm*1.0\n",
    "for ii in range(cm.shape[0]):\n",
    "    cm_row = cm[ii, :]*1.0\n",
    "\n",
    "    cm_row_sum = cm_row.sum()\n",
    "    if cm_row_sum != 0:\n",
    "        cm2[ii, :] = cm_row / cm_row_sum\n",
    "    else:\n",
    "        cm2[ii, :] = np.zeros(cm.shape[1])\n",
    "\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species_list)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format=None, colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "disp2.plot(include_values=True,\n",
    "                     cmap='viridis', ax=ax, xticks_rotation='horizontal',\n",
    "                     values_format='.2f', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
